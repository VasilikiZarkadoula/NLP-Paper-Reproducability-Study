{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d08633c-81c3-4727-b16b-ff96c18b5051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: adapter_transformers==3.1.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 1)) (3.1.0)\n",
      "Collecting fastNLP==0.7.0\n",
      "  Downloading FastNLP-0.7.0.tar.gz (295 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.2/295.2 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fitlog==0.9.13\n",
      "  Downloading fitlog-0.9.13.tar.gz (925 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.2/925.2 kB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy==1.20.3 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 4)) (1.20.3)\n",
      "Requirement already satisfied: transformers==4.23.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 5)) (4.23.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (0.11.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (3.7.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (4.62.3)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from fastNLP==0.7.0->-r requirements.txt (line 2)) (1.12.0+cu116)\n",
      "Requirement already satisfied: prettytable>=0.7.2 in /usr/local/lib/python3.9/dist-packages (from fastNLP==0.7.0->-r requirements.txt (line 2)) (3.4.1)\n",
      "Collecting docopt>=0.6.2\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting flask>=1.0.2\n",
      "  Downloading Flask-2.2.2-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gitpython>=3.1.2 in /usr/local/lib/python3.9/dist-packages (from fitlog==0.9.13->-r requirements.txt (line 3)) (3.1.27)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (4.12.0)\n",
      "Collecting itsdangerous>=2.0\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.9/dist-packages (from flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.9/dist-packages (from flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (8.1.3)\n",
      "Collecting Werkzeug>=2.2.2\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.7/232.7 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from gitpython>=3.1.2->fitlog==0.9.13->-r requirements.txt (line 3)) (4.0.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prettytable>=0.7.2->fastNLP==0.7.0->-r requirements.txt (line 2)) (0.2.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (1.26.10)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.2->fitlog==0.9.13->-r requirements.txt (line 3)) (5.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6.0->flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (3.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from Jinja2>=3.0->flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (2.1.1)\n",
      "Building wheels for collected packages: fastNLP, fitlog, docopt\n",
      "  Building wheel for fastNLP (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fastNLP: filename=FastNLP-0.7.0-py3-none-any.whl size=364728 sha256=03d948788dfac713ecdad460083b4f3fbf34dba93c56882a5d398b07f4eb62ae\n",
      "  Stored in directory: /root/.cache/pip/wheels/41/9e/16/22237f986720cce076fdc608c2bb6bc585326f221247e879db\n",
      "  Building wheel for fitlog (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fitlog: filename=fitlog-0.9.13-py3-none-any.whl size=967466 sha256=ef3acc0a647830f0b329c89aaf48e86653e61e8cc489d3de6b4f3bf535fe48cb\n",
      "  Stored in directory: /root/.cache/pip/wheels/c7/52/61/d760755a6fbb06346a5fd541e5dc7021e7f9ee9cbe6c67edd1\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=9ce09590ad2d344bd04b510bd203e11901941199d52109572df20251ff5586c1\n",
      "  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built fastNLP fitlog docopt\n",
      "Installing collected packages: docopt, Werkzeug, itsdangerous, flask, fastNLP, fitlog\n",
      "  Attempting uninstall: Werkzeug\n",
      "    Found existing installation: Werkzeug 2.1.2\n",
      "    Uninstalling Werkzeug-2.1.2:\n",
      "      Successfully uninstalled Werkzeug-2.1.2\n",
      "Successfully installed Werkzeug-2.2.2 docopt-0.6.2 fastNLP-0.7.0 fitlog-0.9.13 flask-2.2.2 itsdangerous-2.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c065546-7f31-47dd-80ac-5f73b1d3ca73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.12.0+cu116)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.13.0+cu116)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (0.12.0+cu116)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.20.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b55140e-06a7-4f7f-b8f0-ecb8db875951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading vocab.json: 100%|████████████████| 878k/878k [00:00<00:00, 14.0MB/s]\n",
      "Downloading merges.txt: 100%|████████████████| 446k/446k [00:00<00:00, 5.81MB/s]\n",
      "Downloading config.json: 100%|█████████████| 1.68k/1.68k [00:00<00:00, 1.49MB/s]\n",
      "Downloading pytorch_model.bin: 100%|█████████| 532M/532M [00:08<00:00, 65.9MB/s]\n",
      "Read cache from cached_data.bin.\n",
      "# samples: 100000\n",
      "Example:\n",
      "+----------------+----------------+----------------+--------+\n",
      "| refs           | hyps           | labels         | type   |\n",
      "+----------------+----------------+----------------+--------+\n",
      "| a man is pl... | a person is... | -1.18328574... | debias |\n",
      "+----------------+----------------+----------------+--------+\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "input fields after batch(if batch size is 2):\n",
      "\trefs: (1)type:numpy.ndarray (2)dtype:<U32, (3)shape:(2,) \n",
      "\thyps: (1)type:numpy.ndarray (2)dtype:<U33, (3)shape:(2,) \n",
      "\tlabels: (1)type:torch.Tensor (2)dtype:torch.float32, (3)shape:torch.Size([2]) \n",
      "target fields after batch(if batch size is 2):\n",
      "\tlabels: (1)type:torch.Tensor (2)dtype:torch.float32, (3)shape:torch.Size([2]) \n",
      "\n",
      "training epochs started 2023-01-11-20-26-19-829108\n",
      "[epoch:   1 step:   50] train loss: 0.315183 time: 0:00:30\n",
      "[epoch:   1 step:  100] train loss: 0.234843 time: 0:00:59\n",
      "[epoch:   1 step:  150] train loss: 0.227714 time: 0:01:31\n",
      "[epoch:   1 step:  200] train loss: 0.176596 time: 0:02:01\n",
      "[epoch:   1 step:  250] train loss: 0.196625 time: 0:02:32\n",
      "[epoch:   1 step:  300] train loss: 0.174014 time: 0:03:03\n",
      "[epoch:   1 step:  350] train loss: 0.187839 time: 0:03:34\n",
      "[epoch:   1 step:  400] train loss: 0.156467 time: 0:04:04\n",
      "[epoch:   1 step:  450] train loss: 0.165069 time: 0:04:35\n",
      "[epoch:   1 step:  500] train loss: 0.159448 time: 0:05:06\n",
      "[epoch:   1 step:  550] train loss: 0.16488 time: 0:05:37\n",
      "[epoch:   1 step:  600] train loss: 0.13971 time: 0:06:07\n",
      "[epoch:   1 step:  650] train loss: 0.160937 time: 0:06:37\n",
      "[epoch:   1 step:  700] train loss: 0.155887 time: 0:07:09\n",
      "[epoch:   1 step:  750] train loss: 0.161131 time: 0:07:40\n",
      "[epoch:   1 step:  800] train loss: 0.135055 time: 0:08:11\n",
      "[epoch:   1 step:  850] train loss: 0.151472 time: 0:08:42\n",
      "[epoch:   1 step:  900] train loss: 0.134427 time: 0:09:12\n",
      "[epoch:   1 step:  950] train loss: 0.139902 time: 0:09:44\n",
      "[epoch:   1 step: 1000] train loss: 0.142173 time: 0:10:16\n",
      "[epoch:   1 step: 1050] train loss: 0.130482 time: 0:10:48\n",
      "[epoch:   1 step: 1100] train loss: 0.122757 time: 0:11:18\n",
      "[epoch:   1 step: 1150] train loss: 0.12308 time: 0:11:51\n",
      "[epoch:   1 step: 1200] train loss: 0.140536 time: 0:12:22\n",
      "[epoch:   1 step: 1250] train loss: 0.121322 time: 0:12:54\n",
      "[epoch:   1 step: 1300] train loss: 0.132434 time: 0:13:24\n",
      "[epoch:   1 step: 1350] train loss: 0.123241 time: 0:13:55\n",
      "[epoch:   1 step: 1400] train loss: 0.133953 time: 0:14:25\n",
      "[epoch:   1 step: 1450] train loss: 0.121895 time: 0:14:56\n",
      "[epoch:   1 step: 1500] train loss: 0.128295 time: 0:15:27\n",
      "[epoch:   1 step: 1550] train loss: 0.128195 time: 0:15:58\n",
      "[epoch:   1 step: 1600] train loss: 0.109459 time: 0:16:26\n",
      "[epoch:   1 step: 1650] train loss: 0.119139 time: 0:16:57\n",
      "[epoch:   1 step: 1700] train loss: 0.121475 time: 0:17:28\n",
      "[epoch:   1 step: 1750] train loss: 0.118564 time: 0:17:59\n",
      "[epoch:   1 step: 1800] train loss: 0.120126 time: 0:18:30\n",
      "[epoch:   1 step: 1850] train loss: 0.11822 time: 0:19:01\n",
      "[epoch:   1 step: 1900] train loss: 0.119247 time: 0:19:30\n",
      "[epoch:   1 step: 1950] train loss: 0.111728 time: 0:20:02\n",
      "[epoch:   1 step: 2000] train loss: 0.110873 time: 0:20:33\n",
      "[epoch:   1 step: 2050] train loss: 0.104641 time: 0:21:05\n",
      "[epoch:   1 step: 2100] train loss: 0.127163 time: 0:21:34\n",
      "[epoch:   1 step: 2150] train loss: 0.11096 time: 0:22:03\n",
      "[epoch:   1 step: 2200] train loss: 0.120614 time: 0:22:35\n",
      "[epoch:   1 step: 2250] train loss: 0.12398 time: 0:23:04\n",
      "[epoch:   1 step: 2300] train loss: 0.124841 time: 0:23:35\n",
      "[epoch:   1 step: 2350] train loss: 0.100355 time: 0:24:05\n",
      "[epoch:   1 step: 2400] train loss: 0.117281 time: 0:24:36\n",
      "[epoch:   1 step: 2450] train loss: 0.116313 time: 0:25:05\n",
      "[epoch:   1 step: 2500] train loss: 0.11731 time: 0:25:36\n",
      "[epoch:   1 step: 2550] train loss: 0.0984706 time: 0:26:07\n",
      "[epoch:   1 step: 2600] train loss: 0.104602 time: 0:26:37\n",
      "[epoch:   1 step: 2650] train loss: 0.114551 time: 0:27:07\n",
      "[epoch:   1 step: 2700] train loss: 0.111322 time: 0:27:38\n",
      "[epoch:   1 step: 2750] train loss: 0.109638 time: 0:28:10\n",
      "[epoch:   1 step: 2800] train loss: 0.0983479 time: 0:28:40\n",
      "[epoch:   1 step: 2850] train loss: 0.102327 time: 0:29:13\n",
      "[epoch:   1 step: 2900] train loss: 0.105098 time: 0:29:45\n",
      "[epoch:   1 step: 2950] train loss: 0.10647 time: 0:30:15\n",
      "[epoch:   1 step: 3000] train loss: 0.108671 time: 0:30:45\n",
      "[epoch:   1 step: 3050] train loss: 0.106732 time: 0:31:16\n",
      "[epoch:   1 step: 3100] train loss: 0.101465 time: 0:31:47\n",
      "[epoch:   2 step: 3150] train loss: 0.11022 time: 0:32:18\n",
      "[epoch:   2 step: 3200] train loss: 0.106242 time: 0:32:50\n",
      "[epoch:   2 step: 3250] train loss: 0.0983878 time: 0:33:21\n",
      "[epoch:   2 step: 3300] train loss: 0.102505 time: 0:33:51\n",
      "[epoch:   2 step: 3350] train loss: 0.0898839 time: 0:34:21\n",
      "[epoch:   2 step: 3400] train loss: 0.0992854 time: 0:34:52\n",
      "[epoch:   2 step: 3450] train loss: 0.102224 time: 0:35:23\n",
      "[epoch:   2 step: 3500] train loss: 0.09729 time: 0:35:54\n",
      "[epoch:   2 step: 3550] train loss: 0.103347 time: 0:36:25\n",
      "[epoch:   2 step: 3600] train loss: 0.107642 time: 0:36:56\n",
      "[epoch:   2 step: 3650] train loss: 0.0918795 time: 0:37:26\n",
      "[epoch:   2 step: 3700] train loss: 0.0965905 time: 0:37:57\n",
      "[epoch:   2 step: 3750] train loss: 0.0876118 time: 0:38:27\n",
      "[epoch:   2 step: 3800] train loss: 0.106495 time: 0:38:57\n",
      "[epoch:   2 step: 3850] train loss: 0.100263 time: 0:39:27\n",
      "[epoch:   2 step: 3900] train loss: 0.0853693 time: 0:39:58\n",
      "[epoch:   2 step: 3950] train loss: 0.0905711 time: 0:40:30\n",
      "[epoch:   2 step: 4000] train loss: 0.0984726 time: 0:41:01\n",
      "[epoch:   2 step: 4050] train loss: 0.0935264 time: 0:41:32\n",
      "[epoch:   2 step: 4100] train loss: 0.103067 time: 0:42:03\n",
      "[epoch:   2 step: 4150] train loss: 0.101293 time: 0:42:36\n",
      "[epoch:   2 step: 4200] train loss: 0.0885088 time: 0:43:06\n",
      "[epoch:   2 step: 4250] train loss: 0.107924 time: 0:43:38\n",
      "[epoch:   2 step: 4300] train loss: 0.0938316 time: 0:44:10\n",
      "[epoch:   2 step: 4350] train loss: 0.0967219 time: 0:44:41\n",
      "[epoch:   2 step: 4400] train loss: 0.098632 time: 0:45:12\n",
      "[epoch:   2 step: 4450] train loss: 0.0883518 time: 0:45:42\n",
      "[epoch:   2 step: 4500] train loss: 0.0886321 time: 0:46:13\n",
      "[epoch:   2 step: 4550] train loss: 0.0932271 time: 0:46:43\n",
      "[epoch:   2 step: 4600] train loss: 0.0956616 time: 0:47:15\n",
      "[epoch:   2 step: 4650] train loss: 0.0890815 time: 0:47:45\n",
      "[epoch:   2 step: 4700] train loss: 0.0965952 time: 0:48:15\n",
      "[epoch:   2 step: 4750] train loss: 0.0997838 time: 0:48:45\n",
      "[epoch:   2 step: 4800] train loss: 0.0896069 time: 0:49:16\n",
      "[epoch:   2 step: 4850] train loss: 0.0910051 time: 0:49:46\n",
      "[epoch:   2 step: 4900] train loss: 0.0873506 time: 0:50:18\n",
      "[epoch:   2 step: 4950] train loss: 0.0996427 time: 0:50:49\n",
      "[epoch:   2 step: 5000] train loss: 0.0899375 time: 0:51:21\n",
      "[epoch:   2 step: 5050] train loss: 0.094501 time: 0:51:51\n",
      "[epoch:   2 step: 5100] train loss: 0.100266 time: 0:52:22\n",
      "[epoch:   2 step: 5150] train loss: 0.0856575 time: 0:52:52\n",
      "[epoch:   2 step: 5200] train loss: 0.0877903 time: 0:53:23\n",
      "[epoch:   2 step: 5250] train loss: 0.0894691 time: 0:53:54\n",
      "[epoch:   2 step: 5300] train loss: 0.0973208 time: 0:54:23\n",
      "[epoch:   2 step: 5350] train loss: 0.0973931 time: 0:54:53\n",
      "[epoch:   2 step: 5400] train loss: 0.0868752 time: 0:55:23\n",
      "[epoch:   2 step: 5450] train loss: 0.0887562 time: 0:55:53\n",
      "[epoch:   2 step: 5500] train loss: 0.0938212 time: 0:56:25\n",
      "[epoch:   2 step: 5550] train loss: 0.0862991 time: 0:56:56\n",
      "[epoch:   2 step: 5600] train loss: 0.0899593 time: 0:57:26\n",
      "[epoch:   2 step: 5650] train loss: 0.0938437 time: 0:57:58\n",
      "[epoch:   2 step: 5700] train loss: 0.0913353 time: 0:58:28\n",
      "[epoch:   2 step: 5750] train loss: 0.0900393 time: 0:59:00\n",
      "[epoch:   2 step: 5800] train loss: 0.082304 time: 0:59:31\n",
      "[epoch:   2 step: 5850] train loss: 0.077998 time: 1:00:02\n",
      "[epoch:   2 step: 5900] train loss: 0.0820186 time: 1:00:32\n",
      "[epoch:   2 step: 5950] train loss: 0.0841594 time: 1:01:03\n",
      "[epoch:   2 step: 6000] train loss: 0.0738502 time: 1:01:33\n",
      "[epoch:   2 step: 6050] train loss: 0.0947176 time: 1:02:05\n",
      "[epoch:   2 step: 6100] train loss: 0.0923309 time: 1:02:35\n",
      "[epoch:   2 step: 6150] train loss: 0.088792 time: 1:03:05\n",
      "[epoch:   2 step: 6200] train loss: 0.094219 time: 1:03:36\n",
      "[epoch:   2 step: 6250] train loss: 0.0848108 time: 1:04:08\n",
      "[epoch:   3 step: 6300] train loss: 0.0944436 time: 1:04:40\n",
      "[epoch:   3 step: 6350] train loss: 0.0865044 time: 1:05:10\n",
      "[epoch:   3 step: 6400] train loss: 0.0823331 time: 1:05:41\n",
      "[epoch:   3 step: 6450] train loss: 0.0771476 time: 1:06:12\n",
      "[epoch:   3 step: 6500] train loss: 0.0912051 time: 1:06:43\n",
      "[epoch:   3 step: 6550] train loss: 0.083186 time: 1:07:12\n",
      "[epoch:   3 step: 6600] train loss: 0.0817229 time: 1:07:44\n",
      "[epoch:   3 step: 6650] train loss: 0.0857732 time: 1:08:14\n",
      "[epoch:   3 step: 6700] train loss: 0.0830308 time: 1:08:44\n",
      "[epoch:   3 step: 6750] train loss: 0.0811804 time: 1:09:15\n",
      "[epoch:   3 step: 6800] train loss: 0.0785194 time: 1:09:45\n",
      "[epoch:   3 step: 6850] train loss: 0.0844034 time: 1:10:17\n",
      "[epoch:   3 step: 6900] train loss: 0.0795915 time: 1:10:47\n",
      "[epoch:   3 step: 6950] train loss: 0.0803696 time: 1:11:20\n",
      "[epoch:   3 step: 7000] train loss: 0.0776456 time: 1:11:49\n",
      "[epoch:   3 step: 7050] train loss: 0.0825562 time: 1:12:20\n",
      "[epoch:   3 step: 7100] train loss: 0.082892 time: 1:12:51\n",
      "[epoch:   3 step: 7150] train loss: 0.0875236 time: 1:13:20\n",
      "[epoch:   3 step: 7200] train loss: 0.0783442 time: 1:13:50\n",
      "[epoch:   3 step: 7250] train loss: 0.0831496 time: 1:14:21\n",
      "[epoch:   3 step: 7300] train loss: 0.0772183 time: 1:14:51\n",
      "[epoch:   3 step: 7350] train loss: 0.0811177 time: 1:15:22\n",
      "[epoch:   3 step: 7400] train loss: 0.0878788 time: 1:15:52\n",
      "[epoch:   3 step: 7450] train loss: 0.0818439 time: 1:16:21\n",
      "[epoch:   3 step: 7500] train loss: 0.0879491 time: 1:16:52\n",
      "[epoch:   3 step: 7550] train loss: 0.0759535 time: 1:17:22\n",
      "[epoch:   3 step: 7600] train loss: 0.0837433 time: 1:17:52\n",
      "[epoch:   3 step: 7650] train loss: 0.0801141 time: 1:18:23\n",
      "[epoch:   3 step: 7700] train loss: 0.0774124 time: 1:18:54\n",
      "[epoch:   3 step: 7750] train loss: 0.0892681 time: 1:19:26\n",
      "[epoch:   3 step: 7800] train loss: 0.0881004 time: 1:19:57\n",
      "[epoch:   3 step: 7850] train loss: 0.0869224 time: 1:20:29\n",
      "[epoch:   3 step: 7900] train loss: 0.0772327 time: 1:21:00\n",
      "[epoch:   3 step: 7950] train loss: 0.0728411 time: 1:21:31\n",
      "[epoch:   3 step: 8000] train loss: 0.0890093 time: 1:22:02\n",
      "[epoch:   3 step: 8050] train loss: 0.0880527 time: 1:22:33\n",
      "[epoch:   3 step: 8100] train loss: 0.0825277 time: 1:23:03\n",
      "[epoch:   3 step: 8150] train loss: 0.0831563 time: 1:23:33\n",
      "[epoch:   3 step: 8200] train loss: 0.0734194 time: 1:24:05\n",
      "[epoch:   3 step: 8250] train loss: 0.0757541 time: 1:24:36\n",
      "[epoch:   3 step: 8300] train loss: 0.0865641 time: 1:25:07\n",
      "[epoch:   3 step: 8350] train loss: 0.0797813 time: 1:25:39\n",
      "[epoch:   3 step: 8400] train loss: 0.0837169 time: 1:26:09\n",
      "[epoch:   3 step: 8450] train loss: 0.0825577 time: 1:26:40\n",
      "[epoch:   3 step: 8500] train loss: 0.0914704 time: 1:27:11\n",
      "[epoch:   3 step: 8550] train loss: 0.0832568 time: 1:27:43\n",
      "[epoch:   3 step: 8600] train loss: 0.0884331 time: 1:28:15\n",
      "[epoch:   3 step: 8650] train loss: 0.0692553 time: 1:28:46\n",
      "[epoch:   3 step: 8700] train loss: 0.0771114 time: 1:29:17\n",
      "[epoch:   3 step: 8750] train loss: 0.0769088 time: 1:29:47\n",
      "[epoch:   3 step: 8800] train loss: 0.0773902 time: 1:30:18\n",
      "[epoch:   3 step: 8850] train loss: 0.083854 time: 1:30:49\n",
      "[epoch:   3 step: 8900] train loss: 0.0833818 time: 1:31:21\n",
      "[epoch:   3 step: 8950] train loss: 0.0818848 time: 1:31:52\n",
      "[epoch:   3 step: 9000] train loss: 0.0837868 time: 1:32:23\n",
      "[epoch:   3 step: 9050] train loss: 0.0775913 time: 1:32:54\n",
      "[epoch:   3 step: 9100] train loss: 0.0865114 time: 1:33:25\n",
      "[epoch:   3 step: 9150] train loss: 0.0765303 time: 1:33:55\n",
      "[epoch:   3 step: 9200] train loss: 0.0803936 time: 1:34:26\n",
      "[epoch:   3 step: 9250] train loss: 0.0753427 time: 1:34:59\n",
      "[epoch:   3 step: 9300] train loss: 0.0785822 time: 1:35:29\n",
      "[epoch:   3 step: 9350] train loss: 0.0877255 time: 1:36:00\n",
      "[epoch:   4 step: 9400] train loss: 0.0798085 time: 1:36:31\n",
      "[epoch:   4 step: 9450] train loss: 0.068803 time: 1:37:01\n",
      "[epoch:   4 step: 9500] train loss: 0.0775909 time: 1:37:32\n",
      "[epoch:   4 step: 9550] train loss: 0.0714653 time: 1:38:03\n",
      "[epoch:   4 step: 9600] train loss: 0.0758905 time: 1:38:34\n",
      "[epoch:   4 step: 9650] train loss: 0.077835 time: 1:39:06\n",
      "[epoch:   4 step: 9700] train loss: 0.0751643 time: 1:39:36\n",
      "[epoch:   4 step: 9750] train loss: 0.0743481 time: 1:40:06\n",
      "[epoch:   4 step: 9800] train loss: 0.075911 time: 1:40:37\n",
      "[epoch:   4 step: 9850] train loss: 0.0759505 time: 1:41:08\n",
      "[epoch:   4 step: 9900] train loss: 0.0783965 time: 1:41:39\n",
      "[epoch:   4 step: 9950] train loss: 0.0778956 time: 1:42:10\n",
      "[epoch:   4 step: 10000] train loss: 0.0690961 time: 1:42:40\n",
      "[epoch:   4 step: 10050] train loss: 0.0752413 time: 1:43:11\n",
      "[epoch:   4 step: 10100] train loss: 0.0781063 time: 1:43:41\n",
      "[epoch:   4 step: 10150] train loss: 0.0727266 time: 1:44:11\n",
      "[epoch:   4 step: 10200] train loss: 0.0723814 time: 1:44:42\n",
      "[epoch:   4 step: 10250] train loss: 0.0735056 time: 1:45:13\n",
      "[epoch:   4 step: 10300] train loss: 0.0812723 time: 1:45:43\n",
      "[epoch:   4 step: 10350] train loss: 0.0724004 time: 1:46:15\n",
      "[epoch:   4 step: 10400] train loss: 0.0768093 time: 1:46:45\n",
      "[epoch:   4 step: 10450] train loss: 0.070467 time: 1:47:17\n",
      "[epoch:   4 step: 10500] train loss: 0.0732286 time: 1:47:46\n",
      "[epoch:   4 step: 10550] train loss: 0.0767665 time: 1:48:16\n",
      "[epoch:   4 step: 10600] train loss: 0.0719832 time: 1:48:46\n",
      "[epoch:   4 step: 10650] train loss: 0.0700218 time: 1:49:16\n",
      "[epoch:   4 step: 10700] train loss: 0.0792296 time: 1:49:47\n",
      "[epoch:   4 step: 10750] train loss: 0.072116 time: 1:50:17\n",
      "[epoch:   4 step: 10800] train loss: 0.0716737 time: 1:50:48\n",
      "[epoch:   4 step: 10850] train loss: 0.080573 time: 1:51:18\n",
      "[epoch:   4 step: 10900] train loss: 0.0771325 time: 1:51:50\n",
      "[epoch:   4 step: 10950] train loss: 0.0765362 time: 1:52:22\n",
      "[epoch:   4 step: 11000] train loss: 0.0740886 time: 1:52:53\n",
      "[epoch:   4 step: 11050] train loss: 0.0731298 time: 1:53:23\n",
      "[epoch:   4 step: 11100] train loss: 0.075134 time: 1:53:55\n",
      "[epoch:   4 step: 11150] train loss: 0.0789433 time: 1:54:24\n",
      "[epoch:   4 step: 11200] train loss: 0.0767024 time: 1:54:55\n",
      "[epoch:   4 step: 11250] train loss: 0.0702187 time: 1:55:25\n",
      "[epoch:   4 step: 11300] train loss: 0.071982 time: 1:55:55\n",
      "[epoch:   4 step: 11350] train loss: 0.0724261 time: 1:56:27\n",
      "[epoch:   4 step: 11400] train loss: 0.0726618 time: 1:56:58\n",
      "[epoch:   4 step: 11450] train loss: 0.0713931 time: 1:57:31\n",
      "[epoch:   4 step: 11500] train loss: 0.0715975 time: 1:58:02\n",
      "[epoch:   4 step: 11550] train loss: 0.0688086 time: 1:58:32\n",
      "[epoch:   4 step: 11600] train loss: 0.0807855 time: 1:59:02\n",
      "[epoch:   4 step: 11650] train loss: 0.0708833 time: 1:59:34\n",
      "[epoch:   4 step: 11700] train loss: 0.070195 time: 2:00:05\n",
      "[epoch:   4 step: 11750] train loss: 0.081364 time: 2:00:34\n",
      "[epoch:   4 step: 11800] train loss: 0.0709128 time: 2:01:05\n",
      "[epoch:   4 step: 11850] train loss: 0.0774083 time: 2:01:36\n",
      "[epoch:   4 step: 11900] train loss: 0.0792616 time: 2:02:07\n",
      "[epoch:   4 step: 11950] train loss: 0.0802493 time: 2:02:38\n",
      "[epoch:   4 step: 12000] train loss: 0.0709191 time: 2:03:09\n",
      "[epoch:   4 step: 12050] train loss: 0.0739382 time: 2:03:40\n",
      "[epoch:   4 step: 12100] train loss: 0.0788025 time: 2:04:10\n",
      "[epoch:   4 step: 12150] train loss: 0.070847 time: 2:04:41\n",
      "[epoch:   4 step: 12200] train loss: 0.0749757 time: 2:05:11\n",
      "[epoch:   4 step: 12250] train loss: 0.0683767 time: 2:05:42\n",
      "[epoch:   4 step: 12300] train loss: 0.0717466 time: 2:06:13\n",
      "[epoch:   4 step: 12350] train loss: 0.0752427 time: 2:06:45\n",
      "[epoch:   4 step: 12400] train loss: 0.0789258 time: 2:07:16\n",
      "[epoch:   4 step: 12450] train loss: 0.0722423 time: 2:07:47\n",
      "[epoch:   4 step: 12500] train loss: 0.0727736 time: 2:08:18\n"
     ]
    }
   ],
   "source": [
    "!python train_BARTScore.py  --model_type facebook/bart-base --adapter_name debiased-bartscore  --lr 1e-4  --warmup 0.0  --batch_size 32  --n_epochs 4  --seed 42  --device cuda  --logging_steps 150   --data_path bartBase.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b4314-ea55-4f5c-ad13-922a3c9f9a23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
