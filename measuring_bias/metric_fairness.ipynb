{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0112152b-7a4f-482b-bd27-a91ec0107c4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T07:47:02.887185Z",
     "iopub.status.busy": "2023-01-15T07:47:02.886634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting adapter_transformers==3.0.1\n",
      "  Downloading adapter_transformers-3.0.1-py3-none-any.whl (3.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hCollecting bert_score==0.3.11\n",
      "  Downloading bert_score-0.3.11-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets==2.4.0\n",
      "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fairseq==0.9.0\n",
      "  Downloading fairseq-0.9.0.tar.gz (306 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.1/306.1 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting matplotlib==3.5.1\n",
      "  Downloading matplotlib-3.5.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk==3.7 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 6)) (3.7)\n",
      "Collecting numpy==1.23.3\n",
      "  Downloading numpy-1.23.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas==1.4.2\n",
      "  Downloading pandas-1.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting prettytable==3.3.0\n",
      "  Downloading prettytable-3.3.0-py3-none-any.whl (26 kB)\n",
      "Collecting pyemd==0.5.1\n",
      "  Downloading pyemd-0.5.1.tar.gz (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.5/91.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rouge_score==0.1.2\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sacrebleu==2.0.0\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.7/90.7 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece==0.1.96 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 13)) (0.1.96)\n",
      "Collecting torch==1.12.1\n",
      "  Downloading torch-1.12.1-cp39-cp39-manylinux1_x86_64.whl (776.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.4/776.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.22.2\n",
      "  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.0.1->-r requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.0.1->-r requirements.txt (line 1)) (0.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.0.1->-r requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.0.1->-r requirements.txt (line 1)) (3.7.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.0.1->-r requirements.txt (line 1)) (2.28.1)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.0.1->-r requirements.txt (line 1)) (4.64.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.0.1->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.0.1->-r requirements.txt (line 1)) (2022.7.9)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets==2.4.0->-r requirements.txt (line 3)) (0.70.13)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.4.0->-r requirements.txt (line 3)) (8.0.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets==2.4.0->-r requirements.txt (line 3)) (3.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets==2.4.0->-r requirements.txt (line 3)) (3.8.1)\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.9/dist-packages (from datasets==2.4.0->-r requirements.txt (line 3)) (0.3.5.1)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets==2.4.0->-r requirements.txt (line 3)) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.4.0->-r requirements.txt (line 3)) (2022.5.0)\n",
      "Requirement already satisfied: cffi in /usr/local/lib/python3.9/dist-packages (from fairseq==0.9.0->-r requirements.txt (line 4)) (1.15.1)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from fairseq==0.9.0->-r requirements.txt (line 4)) (0.29.30)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->-r requirements.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->-r requirements.txt (line 5)) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->-r requirements.txt (line 5)) (1.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->-r requirements.txt (line 5)) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->-r requirements.txt (line 5)) (9.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->-r requirements.txt (line 5)) (4.34.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk==3.7->-r requirements.txt (line 6)) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk==3.7->-r requirements.txt (line 6)) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas==1.4.2->-r requirements.txt (line 8)) (2022.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prettytable==3.3.0->-r requirements.txt (line 9)) (0.2.5)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from rouge_score==0.1.2->-r requirements.txt (line 11)) (1.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge_score==0.1.2->-r requirements.txt (line 11)) (1.14.0)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: colorama in /usr/lib/python3/dist-packages (from sacrebleu==2.0.0->-r requirements.txt (line 12)) (0.4.3)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.12.1->-r requirements.txt (line 14)) (4.3.0)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->adapter_transformers==3.0.1->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->adapter_transformers==3.0.1->-r requirements.txt (line 1)) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->adapter_transformers==3.0.1->-r requirements.txt (line 1)) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->adapter_transformers==3.0.1->-r requirements.txt (line 1)) (1.26.10)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.4.0->-r requirements.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.4.0->-r requirements.txt (line 3)) (18.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.4.0->-r requirements.txt (line 3)) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.4.0->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.4.0->-r requirements.txt (line 3)) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.4.0->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi->fairseq==0.9.0->-r requirements.txt (line 4)) (2.21)\n",
      "Building wheels for collected packages: fairseq, pyemd, rouge_score, sacremoses\n",
      "  Building wheel for fairseq (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairseq: filename=fairseq-0.9.0-cp39-cp39-linux_x86_64.whl size=2828429 sha256=9da4f081af801b6caa266c01c975201b593b9d3f8d790b544645d7df887a1d49\n",
      "  Stored in directory: /root/.cache/pip/wheels/74/88/07/b9b1ef462797a0ef0a5c9b2468b51afa6bd5535ae32ca7d55f\n",
      "  Building wheel for pyemd (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyemd: filename=pyemd-0.5.1-cp39-cp39-linux_x86_64.whl size=541019 sha256=ad4d353235cf588ed7966f8cf9689b8f17654b2256062d0f06db81838491897b\n",
      "  Stored in directory: /root/.cache/pip/wheels/64/bf/3e/0859be9a0108fc932a29b943792dcafb3b979555cf1bb5add6\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=914ad07577e029b87d45971f0d69d8bc37ed3a9b232887039866b10fc1cd2909\n",
      "  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895242 sha256=28234932c5fb7dc2b56bff274ffe52a741ce3db9d42cfc8e7eb6319854f748ce\n",
      "  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n",
      "Successfully built fairseq pyemd rouge_score sacremoses\n",
      "Installing collected packages: torch, tabulate, sacremoses, prettytable, portalocker, numpy, sacrebleu, rouge_score, pyemd, pandas, matplotlib, huggingface-hub, transformers, fairseq, adapter_transformers, datasets, bert_score\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.0+cu116\n",
      "    Uninstalling torch-1.12.0+cu116:\n",
      "      Successfully uninstalled torch-1.12.0+cu116\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.1\n",
      "    Uninstalling numpy-1.23.1:\n",
      "      Successfully uninstalled numpy-1.23.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.4.3\n",
      "    Uninstalling pandas-1.4.3:\n",
      "      Successfully uninstalled pandas-1.4.3\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.5.2\n",
      "    Uninstalling matplotlib-3.5.2:\n",
      "      Successfully uninstalled matplotlib-3.5.2\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.8.1\n",
      "    Uninstalling huggingface-hub-0.8.1:\n",
      "      Successfully uninstalled huggingface-hub-0.8.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.20.1\n",
      "    Uninstalling transformers-4.20.1:\n",
      "      Successfully uninstalled transformers-4.20.1\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.3.2\n",
      "    Uninstalling datasets-2.3.2:\n",
      "      Successfully uninstalled datasets-2.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.0+cu116 requires torch==1.12.0, but you have torch 1.12.1 which is incompatible.\n",
      "torchaudio 0.12.0+cu116 requires torch==1.12.0, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed adapter_transformers-3.0.1 bert_score-0.3.11 datasets-2.4.0 fairseq-0.9.0 huggingface-hub-0.11.1 matplotlib-3.5.1 numpy-1.23.3 pandas-1.4.2 portalocker-2.6.0 prettytable-3.3.0 pyemd-0.5.1 rouge_score-0.1.2 sacrebleu-2.0.0 sacremoses-0.0.53 tabulate-0.9.0 torch-1.12.1 transformers-4.22.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a36d67c-dfeb-47cf-8071-0f2fce77cf3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T07:29:10.941402Z",
     "iopub.status.busy": "2023-01-14T07:29:10.941049Z",
     "iopub.status.idle": "2023-01-14T07:34:26.970780Z",
     "shell.execute_reply": "2023-01-14T07:34:26.969687Z",
     "shell.execute_reply.started": "2023-01-14T07:29:10.941370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-14 07:29:11--  http://data.statmt.org/prism/m39v1.tar\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://data.statmt.org/prism/m39v1.tar [following]\n",
      "--2023-01-14 07:29:11--  https://data.statmt.org/prism/m39v1.tar\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1493094400 (1.4G) [application/x-tar]\n",
      "Saving to: ‘m39v1.tar.1’\n",
      "\n",
      "m39v1.tar.1         100%[===================>]   1.39G  4.38MB/s    in 5m 15s  \n",
      "\n",
      "2023-01-14 07:34:26 (4.52 MB/s) - ‘m39v1.tar.1’ saved [1493094400/1493094400]\n",
      "\n",
      "--2023-01-14 07:34:26--  http://tar/\n",
      "Resolving tar (tar)... failed: Name or service not known.\n",
      "wget: unable to resolve host address ‘tar’\n",
      "--2023-01-14 07:34:26--  http://xf/\n",
      "Resolving xf (xf)... failed: Name or service not known.\n",
      "wget: unable to resolve host address ‘xf’\n",
      "--2023-01-14 07:34:26--  http://m39v1.tar/\n",
      "Resolving m39v1.tar (m39v1.tar)... failed: Name or service not known.\n",
      "wget: unable to resolve host address ‘m39v1.tar’\n",
      "FINISHED --2023-01-14 07:34:26--\n",
      "Total wall clock time: 5m 16s\n",
      "Downloaded: 1 files, 1.4G in 5m 15s (4.52 MB/s)\n"
     ]
    }
   ],
   "source": [
    "!wget http://data.statmt.org/prism/m39v1.tar tar xf m39v1.tar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4064ca23-bc69-47bd-8753-19364be1e55a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T07:50:39.049665Z",
     "iopub.status.busy": "2023-01-15T07:50:39.049320Z",
     "iopub.status.idle": "2023-01-15T07:50:48.370465Z",
     "shell.execute_reply": "2023-01-15T07:50:48.369719Z",
     "shell.execute_reply.started": "2023-01-15T07:50:39.049637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.25.1\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.25.1) (2.28.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.25.1) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.25.1) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.25.1) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.25.1) (1.23.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.25.1) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.25.1) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.25.1) (0.12.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.25.1) (0.11.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->transformers==4.25.1) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.25.1) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.25.1) (1.26.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.25.1) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.25.1) (2.8)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.22.2\n",
      "    Uninstalling transformers-4.22.2:\n",
      "      Successfully uninstalled transformers-4.22.2\n",
      "Successfully installed transformers-4.25.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.25.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fd48ef2-2ec1-4f5f-a8f7-88b6e952c9b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T09:15:02.817270Z",
     "iopub.status.busy": "2023-01-15T09:15:02.816812Z",
     "iopub.status.idle": "2023-01-15T09:27:18.700685Z",
     "shell.execute_reply": "2023-01-15T09:27:18.699524Z",
     "shell.execute_reply.started": "2023-01-15T09:15:02.817233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "starting calculate bleu\n",
      "starting calculate rouge\n",
      "starting calculate meteor\n",
      "starting calculate nist\n",
      "starting calculate chrf\n",
      "starting calculate bertscore\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "starting calculate moverscore\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "starting calculate bleurt\n",
      "starting calculate prism\n",
      "starting calculate bartscore\n",
      "starting calculate frugalscore\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 29.07ba/s]\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 258\n",
      "  Batch size = 4\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████████████████████████████████████| 65/65 [00:00<00:00, 168.65it/s]\n"
     ]
    }
   ],
   "source": [
    "!python metrics.py  --hyps_file hypsRefs/hypsSocioeconomic.txt --refs_file hypsRefs/refsSocioeconomic.txt --bert_score_model roberta-base --bart_score_model facebook/bart-base-cnn --mover_score_model distilbert-base-uncased --frugal_score_model moussaKam/frugalscore_tiny_bert-base_bert-score --bleurt_score_model Elron/bleurt-base-512 --batch_size 4  --output_file socioecoomic.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02fe2acf-652d-4b39-8c0b-2fa7b8ad7544",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T08:53:11.629128Z",
     "iopub.status.busy": "2023-01-15T08:53:11.628647Z",
     "iopub.status.idle": "2023-01-15T09:12:12.312436Z",
     "shell.execute_reply": "2023-01-15T09:12:12.311299Z",
     "shell.execute_reply.started": "2023-01-15T08:53:11.629088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "starting calculate bleu\n",
      "starting calculate rouge\n",
      "starting calculate meteor\n",
      "starting calculate nist\n",
      "starting calculate chrf\n",
      "starting calculate bertscore\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "starting calculate moverscore\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "starting calculate bleurt\n",
      "starting calculate prism\n",
      "starting calculate bartscore\n",
      "starting calculate frugalscore\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 20.59ba/s]\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 358\n",
      "  Batch size = 4\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████████████████████████████████████| 90/90 [00:00<00:00, 193.35it/s]\n"
     ]
    }
   ],
   "source": [
    "!python3 metrics.py --hyps_file hypsRefs/hypsRace.txt  --refs_file hypsRefs/refsRace.txt  --bert_score_model roberta-large  --bart_score_model facebook/bart-large-cnn  --mover_score_model distilbert-base-uncased --frugal_score_model moussaKam/frugalscore_tiny_bert-base_bert-score  --bleurt_score_model Elron/bleurt-base-512   --batch_size 4    --device cuda  --output_file race.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa2473d7-11ae-4b88-b6ba-fa7176d3f031",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T08:39:03.815957Z",
     "iopub.status.busy": "2023-01-15T08:39:03.815517Z",
     "iopub.status.idle": "2023-01-15T08:47:37.942809Z",
     "shell.execute_reply": "2023-01-15T08:47:37.941511Z",
     "shell.execute_reply.started": "2023-01-15T08:39:03.815919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "starting calculate bleu\n",
      "starting calculate rouge\n",
      "starting calculate meteor\n",
      "starting calculate nist\n",
      "starting calculate chrf\n",
      "starting calculate bertscore\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "starting calculate moverscore\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "starting calculate bleurt\n",
      "starting calculate prism\n",
      "starting calculate bartscore\n",
      "starting calculate frugalscore\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 32.89ba/s]\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 142\n",
      "  Batch size = 4\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 202.11it/s]\n"
     ]
    }
   ],
   "source": [
    "!python3 metrics.py --hyps_file hypsRefs/hypsAge.txt  --refs_file hypsRefs/refsAge.txt  --bert_score_model roberta-large  --bart_score_model facebook/bart-large-cnn  --mover_score_model distilbert-base-uncased --frugal_score_model moussaKam/frugalscore_tiny_bert-base_bert-score  --bleurt_score_model Elron/bleurt-base-512   --batch_size 4    --device cuda  --output_file age.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30e1185c-623f-4094-ae69-08b9bf5e2b90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T08:30:48.658364Z",
     "iopub.status.busy": "2023-01-15T08:30:48.657867Z",
     "iopub.status.idle": "2023-01-15T08:38:11.674395Z",
     "shell.execute_reply": "2023-01-15T08:38:11.673535Z",
     "shell.execute_reply.started": "2023-01-15T08:30:48.658322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "starting calculate bleu\n",
      "starting calculate rouge\n",
      "starting calculate meteor\n",
      "starting calculate nist\n",
      "starting calculate chrf\n",
      "starting calculate bertscore\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "starting calculate moverscore\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "starting calculate bleurt\n",
      "starting calculate prism\n",
      "starting calculate bartscore\n",
      "starting calculate frugalscore\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 35.21ba/s]\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 124\n",
      "  Batch size = 4\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████████████████████████████████████| 31/31 [00:00<00:00, 187.37it/s]\n"
     ]
    }
   ],
   "source": [
    "!python3 metrics.py --hyps_file hypsRefs/hypsAppearance.txt  --refs_file hypsRefs/refsAppearance.txt  --bert_score_model roberta-large  --bart_score_model facebook/bart-large-cnn  --mover_score_model distilbert-base-uncased --frugal_score_model moussaKam/frugalscore_tiny_bert-base_bert-score  --bleurt_score_model Elron/bleurt-base-512   --batch_size 4    --device cuda  --output_file appearance.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba1ad930-e17b-49a3-bfb9-67848602db84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T11:02:49.480454Z",
     "iopub.status.busy": "2023-01-15T11:02:49.479207Z",
     "iopub.status.idle": "2023-01-15T11:15:13.145909Z",
     "shell.execute_reply": "2023-01-15T11:15:13.144840Z",
     "shell.execute_reply.started": "2023-01-15T11:02:49.480399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "starting calculate bleu\n",
      "starting calculate rouge\n",
      "starting calculate meteor\n",
      "starting calculate nist\n",
      "starting calculate chrf\n",
      "starting calculate bertscore\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "starting calculate moverscore\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "starting calculate bleurt\n",
      "starting calculate prism\n",
      "starting calculate bartscore\n",
      "starting calculate frugalscore\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 37.45ba/s]\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 208\n",
      "  Batch size = 4\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████████████████████████████████████| 52/52 [00:00<00:00, 163.40it/s]\n"
     ]
    }
   ],
   "source": [
    "!python3 metrics.py --hyps_file hypsRefs/hypsReligion.txt  --refs_file hypsRefs/refsReligion.txt  --bert_score_model roberta-large  --bart_score_model facebook/bart-large-cnn  --mover_score_model distilbert-base-uncased --frugal_score_model moussaKam/frugalscore_tiny_bert-base_bert-score  --bleurt_score_model Elron/bleurt-base-512   --batch_size 4    --device cuda  --output_file religion.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b2851cb-f590-4786-9b23-5f5666c7e308",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T07:52:54.041186Z",
     "iopub.status.busy": "2023-01-15T07:52:54.040243Z",
     "iopub.status.idle": "2023-01-15T08:26:14.041833Z",
     "shell.execute_reply": "2023-01-15T08:26:14.040732Z",
     "shell.execute_reply.started": "2023-01-15T07:52:54.041146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "starting calculate bleu\n",
      "starting calculate rouge\n",
      "starting calculate meteor\n",
      "starting calculate nist\n",
      "starting calculate chrf\n",
      "starting calculate bertscore\n",
      "Downloading: 100%|██████████████████████████████| 482/482 [00:00<00:00, 455kB/s]\n",
      "Downloading: 100%|███████████████████████████| 899k/899k [00:00<00:00, 11.2MB/s]\n",
      "Downloading: 100%|███████████████████████████| 456k/456k [00:00<00:00, 11.1MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.43G/1.43G [00:31<00:00, 45.9MB/s]\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "starting calculate moverscore\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 19.2kB/s]\n",
      "Downloading: 100%|██████████████████████████████| 483/483 [00:00<00:00, 293kB/s]\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 6.51MB/s]\n",
      "Downloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 9.97MB/s]\n",
      "Downloading: 100%|███████████████████████████| 268M/268M [00:04<00:00, 57.8MB/s]\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "starting calculate bleurt\n",
      "Downloading: 100%|██████████████████████████████| 321/321 [00:00<00:00, 272kB/s]\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 4.04MB/s]\n",
      "Downloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 6.86MB/s]\n",
      "Downloading: 100%|█████████████████████████████| 112/112 [00:00<00:00, 92.9kB/s]\n",
      "Downloading: 100%|██████████████████████████████| 779/779 [00:00<00:00, 496kB/s]\n",
      "Downloading: 100%|███████████████████████████| 438M/438M [00:07<00:00, 55.8MB/s]\n",
      "starting calculate prism\n",
      "starting calculate bartscore\n",
      "Downloading: 100%|███████████████████████████| 899k/899k [00:00<00:00, 11.4MB/s]\n",
      "Downloading: 100%|███████████████████████████| 456k/456k [00:00<00:00, 11.2MB/s]\n",
      "Downloading: 100%|██████████████████████████| 1.58k/1.58k [00:00<00:00, 854kB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.63G/1.63G [00:39<00:00, 41.5MB/s]\n",
      "starting calculate frugalscore\n",
      "Downloading: 100%|██████████████████████████████| 715/715 [00:00<00:00, 302kB/s]\n",
      "Downloading: 100%|█████████████████████████| 17.6M/17.6M [00:00<00:00, 49.2MB/s]\n",
      "Downloading: 100%|██████████████████████████████| 324/324 [00:00<00:00, 191kB/s]\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 4.99MB/s]\n",
      "Downloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 8.57MB/s]\n",
      "Downloading: 100%|█████████████████████████████| 112/112 [00:00<00:00, 64.7kB/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  9.02ba/s]\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2. If sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 792\n",
      "  Batch size = 4\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|████████████████████████████████████████| 198/198 [00:01<00:00, 185.41it/s]\n"
     ]
    }
   ],
   "source": [
    "!python3 metrics.py --hyps_file hypsRefs/hypsGender.txt  --refs_file hypsRefs/refsGender.txt  --bert_score_model roberta-large  --bart_score_model facebook/bart-large-cnn  --mover_score_model distilbert-base-uncased --frugal_score_model moussaKam/frugalscore_tiny_bert-base_bert-score  --bleurt_score_model Elron/bleurt-base-512   --batch_size 4    --device cuda  --output_file gender.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f887b6f2-fb0e-4191-b1ec-2b24e358d250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T07:51:30.508758Z",
     "iopub.status.busy": "2023-01-15T07:51:30.508377Z",
     "iopub.status.idle": "2023-01-15T07:52:47.419370Z",
     "shell.execute_reply": "2023-01-15T07:52:47.418318Z",
     "shell.execute_reply.started": "2023-01-15T07:51:30.508721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.12.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.13.0+cu116)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (0.12.0+cu116)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.23.3)\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu116/torch-1.12.0%2Bcu116-cp39-cp39-linux_x86_64.whl (1904.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 GB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.10)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1\n",
      "    Uninstalling torch-1.12.1:\n",
      "      Successfully uninstalled torch-1.12.1\n",
      "Successfully installed torch-1.12.0+cu116\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
