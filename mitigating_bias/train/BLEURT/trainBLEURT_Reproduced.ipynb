{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "828f4988-a0ff-49c3-9cc4-1a357a3f6bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting adapter_transformers==3.1.0\n",
      "  Downloading adapter_transformers-3.1.0-py3-none-any.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fastNLP==0.7.0\n",
      "  Downloading FastNLP-0.7.0.tar.gz (295 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.2/295.2 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fitlog==0.9.13\n",
      "  Downloading fitlog-0.9.13.tar.gz (925 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.2/925.2 kB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting numpy==1.20.3\n",
      "  Downloading numpy-1.20.3-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.23.1\n",
      "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (2022.7.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (0.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (4.64.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (2.28.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (3.7.1)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from fastNLP==0.7.0->-r requirements.txt (line 2)) (1.12.0+cu116)\n",
      "Collecting prettytable>=0.7.2\n",
      "  Downloading prettytable-3.6.0-py3-none-any.whl (27 kB)\n",
      "Collecting docopt>=0.6.2\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting flask>=1.0.2\n",
      "  Downloading Flask-2.2.2-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gitpython>=3.1.2 in /usr/local/lib/python3.9/dist-packages (from fitlog==0.9.13->-r requirements.txt (line 3)) (3.1.27)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting Werkzeug>=2.2.2\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.7/232.7 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting itsdangerous>=2.0\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (4.12.0)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.9/dist-packages (from flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.9/dist-packages (from flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (8.1.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from gitpython>=3.1.2->fitlog==0.9.13->-r requirements.txt (line 3)) (4.0.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prettytable>=0.7.2->fastNLP==0.7.0->-r requirements.txt (line 2)) (0.2.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (1.26.10)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.2->fitlog==0.9.13->-r requirements.txt (line 3)) (5.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6.0->flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (3.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from Jinja2>=3.0->flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (2.1.1)\n",
      "Building wheels for collected packages: fastNLP, fitlog, docopt\n",
      "  Building wheel for fastNLP (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fastNLP: filename=FastNLP-0.7.0-py3-none-any.whl size=364728 sha256=2ddc55470733b24125d1349f67fe990db9bb498eb0f10c1e32951e30073a1429\n",
      "  Stored in directory: /root/.cache/pip/wheels/41/9e/16/22237f986720cce076fdc608c2bb6bc585326f221247e879db\n",
      "  Building wheel for fitlog (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fitlog: filename=fitlog-0.9.13-py3-none-any.whl size=967466 sha256=c3cc5b8ff045f61f547c9882b024f8f5f6cf0d8cd711813890d5802155656a2d\n",
      "  Stored in directory: /root/.cache/pip/wheels/c7/52/61/d760755a6fbb06346a5fd541e5dc7021e7f9ee9cbe6c67edd1\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=fe21518ae10052da4d351485cff53af961149855471821647d357e07953288bf\n",
      "  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built fastNLP fitlog docopt\n",
      "Installing collected packages: docopt, Werkzeug, prettytable, numpy, itsdangerous, huggingface-hub, flask, fastNLP, transformers, fitlog, adapter_transformers\n",
      "  Attempting uninstall: Werkzeug\n",
      "    Found existing installation: Werkzeug 2.1.2\n",
      "    Uninstalling Werkzeug-2.1.2:\n",
      "      Successfully uninstalled Werkzeug-2.1.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.1\n",
      "    Uninstalling numpy-1.23.1:\n",
      "      Successfully uninstalled numpy-1.23.1\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.8.1\n",
      "    Uninstalling huggingface-hub-0.8.1:\n",
      "      Successfully uninstalled huggingface-hub-0.8.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.20.1\n",
      "    Uninstalling transformers-4.20.1:\n",
      "      Successfully uninstalled transformers-4.20.1\n",
      "Successfully installed Werkzeug-2.2.2 adapter_transformers-3.1.0 docopt-0.6.2 fastNLP-0.7.0 fitlog-0.9.13 flask-2.2.2 huggingface-hub-0.11.1 itsdangerous-2.1.2 numpy-1.20.3 prettytable-3.6.0 transformers-4.23.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1201e92f-f0a0-4b89-9486-9fd5db1389ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.12.0+cu116)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.13.0+cu116)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (0.12.0+cu116)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.20.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.10)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "953fcb53-62f8-4a0f-9512-e490c4902951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read cache from cached_data.bin.\n",
      "# samples: 100000\n",
      "Example:\n",
      "+----------------+----------------+----------------+--------+\n",
      "| refs           | hyps           | labels         | type   |\n",
      "+----------------+----------------+----------------+--------+\n",
      "| a man is pl... | a person is... | 0.668539598... | debias |\n",
      "|                |                |                |        |\n",
      "+----------------+----------------+----------------+--------+\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "input fields after batch(if batch size is 2):\n",
      "\trefs: (1)type:numpy.ndarray (2)dtype:<U32, (3)shape:(2,) \n",
      "\thyps: (1)type:numpy.ndarray (2)dtype:<U33, (3)shape:(2,) \n",
      "\tlabels: (1)type:torch.Tensor (2)dtype:torch.float32, (3)shape:torch.Size([2]) \n",
      "target fields after batch(if batch size is 2):\n",
      "\tlabels: (1)type:torch.Tensor (2)dtype:torch.float32, (3)shape:torch.Size([2]) \n",
      "\n",
      "training epochs started 2023-01-12-18-36-18-092585\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[epoch:   1 step:   50] train loss: 0.132128 time: 0:00:25\n",
      "[epoch:   1 step:  100] train loss: 0.109832 time: 0:00:51\n",
      "[epoch:   1 step:  150] train loss: 0.0920263 time: 0:01:17\n",
      "[epoch:   1 step:  200] train loss: 0.0823385 time: 0:01:43\n",
      "[epoch:   1 step:  250] train loss: 0.0594608 time: 0:02:09\n",
      "[epoch:   1 step:  300] train loss: 0.0592332 time: 0:02:35\n",
      "[epoch:   1 step:  350] train loss: 0.0561566 time: 0:03:01\n",
      "[epoch:   1 step:  400] train loss: 0.0467706 time: 0:03:27\n",
      "[epoch:   1 step:  450] train loss: 0.0403063 time: 0:04:04\n",
      "[epoch:   1 step:  500] train loss: 0.0456615 time: 0:04:51\n",
      "[epoch:   1 step:  550] train loss: 0.0395274 time: 0:05:37\n",
      "[epoch:   1 step:  600] train loss: 0.0387389 time: 0:06:22\n",
      "[epoch:   1 step:  650] train loss: 0.0376573 time: 0:07:09\n",
      "[epoch:   1 step:  700] train loss: 0.036207 time: 0:07:54\n",
      "[epoch:   1 step:  750] train loss: 0.0357189 time: 0:08:40\n",
      "[epoch:   1 step:  800] train loss: 0.0343931 time: 0:09:23\n",
      "[epoch:   1 step:  850] train loss: 0.0331581 time: 0:10:10\n",
      "[epoch:   1 step:  900] train loss: 0.0390539 time: 0:10:53\n",
      "[epoch:   1 step:  950] train loss: 0.036211 time: 0:11:39\n",
      "[epoch:   1 step: 1000] train loss: 0.0306872 time: 0:12:26\n",
      "[epoch:   1 step: 1050] train loss: 0.0315749 time: 0:13:10\n",
      "[epoch:   1 step: 1100] train loss: 0.0316964 time: 0:13:51\n",
      "[epoch:   1 step: 1150] train loss: 0.0270669 time: 0:14:32\n",
      "[epoch:   1 step: 1200] train loss: 0.0268924 time: 0:15:13\n",
      "[epoch:   1 step: 1250] train loss: 0.030756 time: 0:15:54\n",
      "[epoch:   1 step: 1300] train loss: 0.0292502 time: 0:16:35\n",
      "[epoch:   1 step: 1350] train loss: 0.0246849 time: 0:17:16\n",
      "[epoch:   1 step: 1400] train loss: 0.0254456 time: 0:17:58\n",
      "[epoch:   1 step: 1450] train loss: 0.0280177 time: 0:18:38\n",
      "[epoch:   1 step: 1500] train loss: 0.0326035 time: 0:19:19\n",
      "[epoch:   1 step: 1550] train loss: 0.0301173 time: 0:20:01\n",
      "[epoch:   1 step: 1600] train loss: 0.0250443 time: 0:20:45\n",
      "[epoch:   1 step: 1650] train loss: 0.0298192 time: 0:21:28\n",
      "[epoch:   1 step: 1700] train loss: 0.0257415 time: 0:22:12\n",
      "[epoch:   1 step: 1750] train loss: 0.0262078 time: 0:22:58\n",
      "[epoch:   1 step: 1800] train loss: 0.0267608 time: 0:23:43\n",
      "[epoch:   1 step: 1850] train loss: 0.0288216 time: 0:24:26\n",
      "[epoch:   1 step: 1900] train loss: 0.0232318 time: 0:25:08\n",
      "[epoch:   1 step: 1950] train loss: 0.0253442 time: 0:25:43\n",
      "[epoch:   1 step: 2000] train loss: 0.0215311 time: 0:26:26\n",
      "[epoch:   1 step: 2050] train loss: 0.0202587 time: 0:27:02\n",
      "[epoch:   1 step: 2100] train loss: 0.0246703 time: 0:27:44\n",
      "[epoch:   1 step: 2150] train loss: 0.0222893 time: 0:28:24\n",
      "[epoch:   1 step: 2200] train loss: 0.0220675 time: 0:29:04\n",
      "[epoch:   1 step: 2250] train loss: 0.0231522 time: 0:29:49\n",
      "[epoch:   1 step: 2300] train loss: 0.0223411 time: 0:30:31\n",
      "[epoch:   1 step: 2350] train loss: 0.0236087 time: 0:31:12\n",
      "[epoch:   1 step: 2400] train loss: 0.0224816 time: 0:31:55\n",
      "[epoch:   1 step: 2450] train loss: 0.0196983 time: 0:32:34\n",
      "[epoch:   1 step: 2500] train loss: 0.0211776 time: 0:33:14\n",
      "[epoch:   1 step: 2550] train loss: 0.0208653 time: 0:33:51\n",
      "[epoch:   1 step: 2600] train loss: 0.0192758 time: 0:34:30\n",
      "[epoch:   1 step: 2650] train loss: 0.020298 time: 0:35:12\n",
      "[epoch:   1 step: 2700] train loss: 0.022087 time: 0:35:50\n",
      "[epoch:   1 step: 2750] train loss: 0.0227387 time: 0:36:33\n",
      "[epoch:   1 step: 2800] train loss: 0.0173258 time: 0:37:14\n",
      "[epoch:   1 step: 2850] train loss: 0.0196985 time: 0:37:58\n",
      "[epoch:   1 step: 2900] train loss: 0.0195581 time: 0:38:39\n",
      "[epoch:   1 step: 2950] train loss: 0.0188325 time: 0:39:22\n",
      "[epoch:   1 step: 3000] train loss: 0.0199878 time: 0:40:03\n",
      "[epoch:   1 step: 3050] train loss: 0.018967 time: 0:40:46\n",
      "[epoch:   1 step: 3100] train loss: 0.0231477 time: 0:41:27\n",
      "[epoch:   1 step: 3150] train loss: 0.0197674 time: 0:42:10\n",
      "[epoch:   1 step: 3200] train loss: 0.0216418 time: 0:42:53\n",
      "[epoch:   1 step: 3250] train loss: 0.0179308 time: 0:43:38\n",
      "[epoch:   1 step: 3300] train loss: 0.0206537 time: 0:44:22\n",
      "[epoch:   1 step: 3350] train loss: 0.0210023 time: 0:45:07\n",
      "[epoch:   1 step: 3400] train loss: 0.0173326 time: 0:45:51\n",
      "[epoch:   1 step: 3450] train loss: 0.0184087 time: 0:46:36\n",
      "[epoch:   1 step: 3500] train loss: 0.018525 time: 0:47:15\n",
      "[epoch:   1 step: 3550] train loss: 0.0179716 time: 0:47:58\n",
      "[epoch:   1 step: 3600] train loss: 0.0163568 time: 0:48:43\n",
      "[epoch:   1 step: 3650] train loss: 0.0170419 time: 0:49:27\n",
      "[epoch:   1 step: 3700] train loss: 0.0191031 time: 0:50:12\n",
      "[epoch:   1 step: 3750] train loss: 0.0195216 time: 0:50:57\n",
      "[epoch:   1 step: 3800] train loss: 0.0167949 time: 0:51:41\n",
      "[epoch:   1 step: 3850] train loss: 0.0189671 time: 0:52:26\n",
      "[epoch:   1 step: 3900] train loss: 0.0152775 time: 0:53:11\n",
      "[epoch:   1 step: 3950] train loss: 0.0180698 time: 0:53:55\n",
      "[epoch:   1 step: 4000] train loss: 0.0169217 time: 0:54:38\n",
      "[epoch:   1 step: 4050] train loss: 0.017853 time: 0:55:05\n",
      "[epoch:   1 step: 4100] train loss: 0.0172687 time: 0:55:31\n",
      "[epoch:   1 step: 4150] train loss: 0.0167314 time: 0:55:57\n",
      "[epoch:   1 step: 4200] train loss: 0.0181338 time: 0:56:22\n",
      "[epoch:   1 step: 4250] train loss: 0.0151954 time: 0:56:48\n",
      "[epoch:   1 step: 4300] train loss: 0.0167634 time: 0:57:14\n",
      "[epoch:   1 step: 4350] train loss: 0.0186397 time: 0:57:40\n",
      "[epoch:   1 step: 4400] train loss: 0.0157358 time: 0:58:06\n",
      "[epoch:   1 step: 4450] train loss: 0.0173345 time: 0:58:32\n",
      "[epoch:   1 step: 4500] train loss: 0.0167978 time: 0:58:58\n",
      "[epoch:   1 step: 4550] train loss: 0.0163253 time: 0:59:24\n",
      "[epoch:   1 step: 4600] train loss: 0.0171109 time: 0:59:50\n",
      "[epoch:   1 step: 4650] train loss: 0.0166094 time: 1:00:16\n",
      "[epoch:   1 step: 4700] train loss: 0.0158324 time: 1:00:42\n",
      "[epoch:   1 step: 4750] train loss: 0.0164126 time: 1:01:08\n",
      "[epoch:   1 step: 4800] train loss: 0.0178447 time: 1:01:34\n",
      "[epoch:   1 step: 4850] train loss: 0.0166973 time: 1:01:59\n",
      "[epoch:   1 step: 4900] train loss: 0.015132 time: 1:02:25\n",
      "[epoch:   1 step: 4950] train loss: 0.0168632 time: 1:02:51\n",
      "[epoch:   1 step: 5000] train loss: 0.0165571 time: 1:03:17\n",
      "[epoch:   1 step: 5050] train loss: 0.0166635 time: 1:03:43\n",
      "[epoch:   1 step: 5100] train loss: 0.0163496 time: 1:04:09\n",
      "[epoch:   1 step: 5150] train loss: 0.016153 time: 1:04:35\n",
      "[epoch:   1 step: 5200] train loss: 0.0158916 time: 1:05:01\n",
      "[epoch:   1 step: 5250] train loss: 0.0133731 time: 1:05:27\n",
      "[epoch:   1 step: 5300] train loss: 0.0161933 time: 1:05:53\n",
      "[epoch:   1 step: 5350] train loss: 0.0148001 time: 1:06:19\n",
      "[epoch:   1 step: 5400] train loss: 0.0142593 time: 1:06:45\n",
      "[epoch:   1 step: 5450] train loss: 0.0164804 time: 1:07:10\n",
      "[epoch:   1 step: 5500] train loss: 0.0181484 time: 1:07:36\n",
      "[epoch:   1 step: 5550] train loss: 0.0167223 time: 1:08:02\n",
      "[epoch:   1 step: 5600] train loss: 0.0156947 time: 1:08:28\n",
      "[epoch:   1 step: 5650] train loss: 0.0156903 time: 1:08:54\n",
      "[epoch:   1 step: 5700] train loss: 0.0146405 time: 1:09:20\n",
      "[epoch:   1 step: 5750] train loss: 0.0158382 time: 1:09:46\n",
      "[epoch:   1 step: 5800] train loss: 0.0144826 time: 1:10:12\n",
      "[epoch:   1 step: 5850] train loss: 0.014388 time: 1:10:38\n",
      "[epoch:   1 step: 5900] train loss: 0.0155526 time: 1:11:04\n",
      "[epoch:   1 step: 5950] train loss: 0.0172985 time: 1:11:30\n",
      "[epoch:   1 step: 6000] train loss: 0.0157016 time: 1:11:56\n",
      "[epoch:   1 step: 6050] train loss: 0.015329 time: 1:12:22\n",
      "[epoch:   1 step: 6100] train loss: 0.0147695 time: 1:12:48\n",
      "[epoch:   1 step: 6150] train loss: 0.0142419 time: 1:13:14\n",
      "[epoch:   1 step: 6200] train loss: 0.0141824 time: 1:13:40\n",
      "[epoch:   1 step: 6250] train loss: 0.0138617 time: 1:14:06\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[epoch:   2 step: 6300] train loss: 0.0155191 time: 1:14:32\n",
      "[epoch:   2 step: 6350] train loss: 0.0148938 time: 1:14:57\n",
      "[epoch:   2 step: 6400] train loss: 0.0137252 time: 1:15:23\n",
      "[epoch:   2 step: 6450] train loss: 0.0135177 time: 1:15:49\n",
      "[epoch:   2 step: 6500] train loss: 0.0154026 time: 1:16:15\n",
      "[epoch:   2 step: 6550] train loss: 0.0124963 time: 1:16:41\n",
      "[epoch:   2 step: 6600] train loss: 0.0131315 time: 1:17:07\n",
      "[epoch:   2 step: 6650] train loss: 0.0122095 time: 1:17:33\n",
      "[epoch:   2 step: 6700] train loss: 0.0130728 time: 1:17:59\n",
      "[epoch:   2 step: 6750] train loss: 0.0133326 time: 1:18:25\n",
      "[epoch:   2 step: 6800] train loss: 0.0134636 time: 1:18:51\n",
      "[epoch:   2 step: 6850] train loss: 0.0123244 time: 1:19:17\n",
      "[epoch:   2 step: 6900] train loss: 0.0144213 time: 1:19:42\n",
      "[epoch:   2 step: 6950] train loss: 0.0147372 time: 1:20:08\n",
      "[epoch:   2 step: 7000] train loss: 0.0141533 time: 1:20:34\n",
      "[epoch:   2 step: 7050] train loss: 0.0139734 time: 1:21:00\n",
      "[epoch:   2 step: 7100] train loss: 0.0129801 time: 1:21:26\n",
      "[epoch:   2 step: 7150] train loss: 0.0115205 time: 1:21:52\n",
      "[epoch:   2 step: 7200] train loss: 0.0138092 time: 1:22:18\n",
      "[epoch:   2 step: 7250] train loss: 0.0126024 time: 1:22:44\n",
      "[epoch:   2 step: 7300] train loss: 0.0130397 time: 1:23:10\n",
      "[epoch:   2 step: 7350] train loss: 0.0135805 time: 1:23:36\n",
      "[epoch:   2 step: 7400] train loss: 0.0129008 time: 1:24:02\n",
      "[epoch:   2 step: 7450] train loss: 0.0127748 time: 1:24:28\n",
      "[epoch:   2 step: 7500] train loss: 0.0129641 time: 1:24:53\n",
      "[epoch:   2 step: 7550] train loss: 0.0137408 time: 1:25:19\n",
      "[epoch:   2 step: 7600] train loss: 0.0136893 time: 1:25:45\n",
      "[epoch:   2 step: 7650] train loss: 0.011629 time: 1:26:11\n",
      "[epoch:   2 step: 7700] train loss: 0.0117679 time: 1:26:37\n",
      "[epoch:   2 step: 7750] train loss: 0.0130219 time: 1:27:03\n",
      "[epoch:   2 step: 7800] train loss: 0.0128878 time: 1:27:29\n",
      "[epoch:   2 step: 7850] train loss: 0.0138403 time: 1:27:55\n",
      "[epoch:   2 step: 7900] train loss: 0.0146085 time: 1:28:21\n",
      "[epoch:   2 step: 7950] train loss: 0.0117233 time: 1:28:47\n",
      "[epoch:   2 step: 8000] train loss: 0.0143421 time: 1:29:13\n",
      "[epoch:   2 step: 8050] train loss: 0.0130739 time: 1:29:39\n",
      "[epoch:   2 step: 8100] train loss: 0.0133964 time: 1:30:05\n",
      "[epoch:   2 step: 8150] train loss: 0.014965 time: 1:30:31\n",
      "[epoch:   2 step: 8200] train loss: 0.0132582 time: 1:30:56\n",
      "[epoch:   2 step: 8250] train loss: 0.0130562 time: 1:31:22\n",
      "[epoch:   2 step: 8300] train loss: 0.0128936 time: 1:31:48\n",
      "[epoch:   2 step: 8350] train loss: 0.0129257 time: 1:32:14\n",
      "[epoch:   2 step: 8400] train loss: 0.0115327 time: 1:32:40\n",
      "[epoch:   2 step: 8450] train loss: 0.0120201 time: 1:33:06\n",
      "[epoch:   2 step: 8500] train loss: 0.0126111 time: 1:33:32\n",
      "[epoch:   2 step: 8550] train loss: 0.0117974 time: 1:33:58\n",
      "[epoch:   2 step: 8600] train loss: 0.013008 time: 1:34:24\n",
      "[epoch:   2 step: 8650] train loss: 0.0125373 time: 1:34:50\n",
      "[epoch:   2 step: 8700] train loss: 0.0123144 time: 1:35:15\n",
      "[epoch:   2 step: 8750] train loss: 0.0122494 time: 1:35:41\n",
      "[epoch:   2 step: 8800] train loss: 0.0112495 time: 1:36:07\n",
      "[epoch:   2 step: 8850] train loss: 0.0118022 time: 1:36:33\n",
      "[epoch:   2 step: 8900] train loss: 0.0123401 time: 1:36:59\n",
      "[epoch:   2 step: 8950] train loss: 0.0120517 time: 1:37:25\n",
      "[epoch:   2 step: 9000] train loss: 0.0132132 time: 1:37:51\n",
      "[epoch:   2 step: 9050] train loss: 0.0103435 time: 1:38:17\n",
      "[epoch:   2 step: 9100] train loss: 0.0124728 time: 1:38:43\n",
      "[epoch:   2 step: 9150] train loss: 0.0124096 time: 1:39:09\n",
      "[epoch:   2 step: 9200] train loss: 0.0115775 time: 1:39:35\n",
      "[epoch:   2 step: 9250] train loss: 0.0118809 time: 1:40:00\n",
      "[epoch:   2 step: 9300] train loss: 0.012367 time: 1:40:26\n",
      "[epoch:   2 step: 9350] train loss: 0.0133518 time: 1:40:52\n",
      "[epoch:   2 step: 9400] train loss: 0.0125898 time: 1:41:18\n",
      "[epoch:   2 step: 9450] train loss: 0.0117291 time: 1:41:44\n",
      "[epoch:   2 step: 9500] train loss: 0.0115067 time: 1:42:10\n",
      "[epoch:   2 step: 9550] train loss: 0.011576 time: 1:42:36\n",
      "[epoch:   2 step: 9600] train loss: 0.0112847 time: 1:43:02\n",
      "[epoch:   2 step: 9650] train loss: 0.0129551 time: 1:43:28\n",
      "[epoch:   2 step: 9700] train loss: 0.0121587 time: 1:43:54\n",
      "[epoch:   2 step: 9750] train loss: 0.0120062 time: 1:44:19\n",
      "[epoch:   2 step: 9800] train loss: 0.0119391 time: 1:44:45\n",
      "[epoch:   2 step: 9850] train loss: 0.0117468 time: 1:45:11\n",
      "[epoch:   2 step: 9900] train loss: 0.0123324 time: 1:45:37\n",
      "[epoch:   2 step: 9950] train loss: 0.0117615 time: 1:46:03\n",
      "[epoch:   2 step: 10000] train loss: 0.0106781 time: 1:46:29\n",
      "[epoch:   2 step: 10050] train loss: 0.011386 time: 1:46:55\n",
      "[epoch:   2 step: 10100] train loss: 0.0119793 time: 1:47:21\n",
      "[epoch:   2 step: 10150] train loss: 0.0120192 time: 1:47:47\n",
      "[epoch:   2 step: 10200] train loss: 0.0139882 time: 1:48:13\n",
      "[epoch:   2 step: 10250] train loss: 0.0126011 time: 1:48:39\n",
      "[epoch:   2 step: 10300] train loss: 0.0121918 time: 1:49:05\n",
      "[epoch:   2 step: 10350] train loss: 0.0122826 time: 1:49:31\n",
      "[epoch:   2 step: 10400] train loss: 0.0125705 time: 1:49:56\n",
      "[epoch:   2 step: 10450] train loss: 0.0117063 time: 1:50:22\n",
      "[epoch:   2 step: 10500] train loss: 0.0116679 time: 1:50:48\n",
      "[epoch:   2 step: 10550] train loss: 0.0129628 time: 1:51:14\n",
      "[epoch:   2 step: 10600] train loss: 0.0112612 time: 1:51:40\n",
      "[epoch:   2 step: 10650] train loss: 0.0117022 time: 1:52:06\n",
      "[epoch:   2 step: 10700] train loss: 0.0129067 time: 1:52:32\n",
      "[epoch:   2 step: 10750] train loss: 0.012533 time: 1:52:58\n",
      "[epoch:   2 step: 10800] train loss: 0.0129422 time: 1:53:24\n",
      "[epoch:   2 step: 10850] train loss: 0.0111656 time: 1:53:50\n",
      "[epoch:   2 step: 10900] train loss: 0.0109769 time: 1:54:15\n",
      "[epoch:   2 step: 10950] train loss: 0.00994671 time: 1:54:41\n",
      "[epoch:   2 step: 11000] train loss: 0.0110976 time: 1:55:07\n",
      "[epoch:   2 step: 11050] train loss: 0.012545 time: 1:55:33\n",
      "[epoch:   2 step: 11100] train loss: 0.0121897 time: 1:55:59\n",
      "[epoch:   2 step: 11150] train loss: 0.0118688 time: 1:56:25\n",
      "[epoch:   2 step: 11200] train loss: 0.0107245 time: 1:56:51\n",
      "[epoch:   2 step: 11250] train loss: 0.0107445 time: 1:57:17\n",
      "[epoch:   2 step: 11300] train loss: 0.0106928 time: 1:57:43\n",
      "[epoch:   2 step: 11350] train loss: 0.011259 time: 1:58:09\n",
      "[epoch:   2 step: 11400] train loss: 0.0118028 time: 1:58:34\n",
      "[epoch:   2 step: 11450] train loss: 0.014305 time: 1:59:00\n",
      "[epoch:   2 step: 11500] train loss: 0.0122969 time: 1:59:26\n",
      "[epoch:   2 step: 11550] train loss: 0.0123954 time: 1:59:52\n",
      "[epoch:   2 step: 11600] train loss: 0.0117298 time: 2:00:18\n",
      "[epoch:   2 step: 11650] train loss: 0.0113852 time: 2:00:44\n",
      "[epoch:   2 step: 11700] train loss: 0.0116832 time: 2:01:10\n",
      "[epoch:   2 step: 11750] train loss: 0.0111147 time: 2:01:36\n",
      "[epoch:   2 step: 11800] train loss: 0.0114094 time: 2:02:02\n",
      "[epoch:   2 step: 11850] train loss: 0.0104341 time: 2:02:28\n",
      "[epoch:   2 step: 11900] train loss: 0.0110636 time: 2:02:54\n",
      "[epoch:   2 step: 11950] train loss: 0.0109444 time: 2:03:20\n",
      "[epoch:   2 step: 12000] train loss: 0.0117427 time: 2:03:45\n",
      "[epoch:   2 step: 12050] train loss: 0.0115408 time: 2:04:11\n",
      "[epoch:   2 step: 12100] train loss: 0.0114547 time: 2:04:37\n",
      "[epoch:   2 step: 12150] train loss: 0.011383 time: 2:05:03\n",
      "[epoch:   2 step: 12200] train loss: 0.0109294 time: 2:05:29\n",
      "[epoch:   2 step: 12250] train loss: 0.0112048 time: 2:05:55\n",
      "[epoch:   2 step: 12300] train loss: 0.0109084 time: 2:06:21\n",
      "[epoch:   2 step: 12350] train loss: 0.0113177 time: 2:06:47\n",
      "[epoch:   2 step: 12400] train loss: 0.0114678 time: 2:07:17\n",
      "[epoch:   2 step: 12450] train loss: 0.0101134 time: 2:07:54\n",
      "[epoch:   2 step: 12500] train loss: 0.011945 time: 2:08:33\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[epoch:   3 step: 12550] train loss: 0.0106558 time: 2:09:10\n",
      "[epoch:   3 step: 12600] train loss: 0.0115376 time: 2:09:47\n",
      "[epoch:   3 step: 12650] train loss: 0.0107192 time: 2:10:24\n",
      "[epoch:   3 step: 12700] train loss: 0.01047 time: 2:11:01\n",
      "[epoch:   3 step: 12750] train loss: 0.00938356 time: 2:11:37\n",
      "[epoch:   3 step: 12800] train loss: 0.00947251 time: 2:12:10\n",
      "[epoch:   3 step: 12850] train loss: 0.0111704 time: 2:12:44\n",
      "[epoch:   3 step: 12900] train loss: 0.0101382 time: 2:13:18\n",
      "[epoch:   3 step: 12950] train loss: 0.00990591 time: 2:13:51\n",
      "[epoch:   3 step: 13000] train loss: 0.0104147 time: 2:14:27\n",
      "[epoch:   3 step: 13050] train loss: 0.00992232 time: 2:15:01\n",
      "[epoch:   3 step: 13100] train loss: 0.00911459 time: 2:15:34\n",
      "[epoch:   3 step: 13150] train loss: 0.0111061 time: 2:16:09\n",
      "[epoch:   3 step: 13200] train loss: 0.00969692 time: 2:16:44\n",
      "[epoch:   3 step: 13250] train loss: 0.00926427 time: 2:17:21\n",
      "[epoch:   3 step: 13300] train loss: 0.0108012 time: 2:17:58\n",
      "[epoch:   3 step: 13350] train loss: 0.011099 time: 2:18:30\n",
      "[epoch:   3 step: 13400] train loss: 0.0101506 time: 2:19:02\n",
      "[epoch:   3 step: 13450] train loss: 0.00998021 time: 2:19:34\n",
      "[epoch:   3 step: 13500] train loss: 0.0107083 time: 2:20:08\n",
      "[epoch:   3 step: 13550] train loss: 0.00932692 time: 2:20:42\n",
      "[epoch:   3 step: 13600] train loss: 0.00985213 time: 2:21:17\n",
      "[epoch:   3 step: 13650] train loss: 0.00964094 time: 2:21:51\n",
      "[epoch:   3 step: 13700] train loss: 0.00950408 time: 2:22:26\n",
      "[epoch:   3 step: 13750] train loss: 0.011758 time: 2:22:59\n",
      "[epoch:   3 step: 13800] train loss: 0.0100973 time: 2:23:32\n",
      "[epoch:   3 step: 13850] train loss: 0.00948409 time: 2:24:03\n",
      "[epoch:   3 step: 13900] train loss: 0.0101098 time: 2:24:37\n",
      "[epoch:   3 step: 13950] train loss: 0.0104304 time: 2:25:10\n",
      "[epoch:   3 step: 14000] train loss: 0.0112542 time: 2:25:44\n",
      "[epoch:   3 step: 14050] train loss: 0.00944764 time: 2:26:19\n",
      "[epoch:   3 step: 14100] train loss: 0.0103129 time: 2:26:54\n",
      "[epoch:   3 step: 14150] train loss: 0.0106444 time: 2:27:28\n",
      "[epoch:   3 step: 14200] train loss: 0.0112484 time: 2:28:02\n",
      "[epoch:   3 step: 14250] train loss: 0.010845 time: 2:28:37\n",
      "[epoch:   3 step: 14300] train loss: 0.00866557 time: 2:29:12\n",
      "[epoch:   3 step: 14350] train loss: 0.0110207 time: 2:29:48\n",
      "[epoch:   3 step: 14400] train loss: 0.0109942 time: 2:30:26\n",
      "[epoch:   3 step: 14450] train loss: 0.00977225 time: 2:31:01\n",
      "[epoch:   3 step: 14500] train loss: 0.0102025 time: 2:31:37\n",
      "[epoch:   3 step: 14550] train loss: 0.00981201 time: 2:32:12\n",
      "[epoch:   3 step: 14600] train loss: 0.00937815 time: 2:32:48\n",
      "[epoch:   3 step: 14650] train loss: 0.00979834 time: 2:33:25\n",
      "[epoch:   3 step: 14700] train loss: 0.00913226 time: 2:34:01\n",
      "[epoch:   3 step: 14750] train loss: 0.0109075 time: 2:34:36\n",
      "[epoch:   3 step: 14800] train loss: 0.00969206 time: 2:35:06\n",
      "[epoch:   3 step: 14850] train loss: 0.010182 time: 2:35:32\n",
      "[epoch:   3 step: 14900] train loss: 0.0090859 time: 2:35:58\n",
      "[epoch:   3 step: 14950] train loss: 0.0107514 time: 2:36:24\n",
      "[epoch:   3 step: 15000] train loss: 0.00938926 time: 2:36:50\n",
      "[epoch:   3 step: 15050] train loss: 0.0105881 time: 2:37:16\n",
      "[epoch:   3 step: 15100] train loss: 0.00962369 time: 2:37:42\n",
      "[epoch:   3 step: 15150] train loss: 0.00879746 time: 2:38:08\n",
      "[epoch:   3 step: 15200] train loss: 0.0101626 time: 2:38:34\n",
      "[epoch:   3 step: 15250] train loss: 0.0091828 time: 2:39:00\n",
      "[epoch:   3 step: 15300] train loss: 0.0103424 time: 2:39:26\n",
      "[epoch:   3 step: 15350] train loss: 0.010524 time: 2:39:52\n",
      "[epoch:   3 step: 15400] train loss: 0.0104623 time: 2:40:18\n",
      "[epoch:   3 step: 15450] train loss: 0.00962419 time: 2:40:44\n",
      "[epoch:   3 step: 15500] train loss: 0.0096104 time: 2:41:10\n",
      "[epoch:   3 step: 15550] train loss: 0.0103698 time: 2:41:37\n",
      "[epoch:   3 step: 15600] train loss: 0.0106384 time: 2:42:03\n",
      "[epoch:   3 step: 15650] train loss: 0.00984805 time: 2:42:29\n",
      "[epoch:   3 step: 15700] train loss: 0.0105567 time: 2:42:56\n",
      "[epoch:   3 step: 15750] train loss: 0.0101756 time: 2:43:22\n",
      "[epoch:   3 step: 15800] train loss: 0.010421 time: 2:43:49\n",
      "[epoch:   3 step: 15850] train loss: 0.0102392 time: 2:44:15\n",
      "[epoch:   3 step: 15900] train loss: 0.00959742 time: 2:44:42\n",
      "[epoch:   3 step: 15950] train loss: 0.00977806 time: 2:45:08\n",
      "[epoch:   3 step: 16000] train loss: 0.00973657 time: 2:45:34\n",
      "[epoch:   3 step: 16050] train loss: 0.011429 time: 2:46:01\n",
      "[epoch:   3 step: 16100] train loss: 0.00952779 time: 2:46:27\n",
      "[epoch:   3 step: 16150] train loss: 0.00977164 time: 2:46:54\n",
      "[epoch:   3 step: 16200] train loss: 0.00953163 time: 2:47:20\n",
      "[epoch:   3 step: 16250] train loss: 0.0102588 time: 2:47:47\n",
      "[epoch:   3 step: 16300] train loss: 0.0081088 time: 2:48:13\n",
      "[epoch:   3 step: 16350] train loss: 0.00923075 time: 2:48:39\n",
      "[epoch:   3 step: 16400] train loss: 0.0109366 time: 2:49:06\n",
      "[epoch:   3 step: 16450] train loss: 0.0106289 time: 2:49:32\n",
      "[epoch:   3 step: 16500] train loss: 0.0107494 time: 2:49:59\n",
      "[epoch:   3 step: 16550] train loss: 0.00898327 time: 2:50:25\n",
      "[epoch:   3 step: 16600] train loss: 0.00964831 time: 2:50:52\n",
      "[epoch:   3 step: 16650] train loss: 0.00948736 time: 2:51:18\n",
      "[epoch:   3 step: 16700] train loss: 0.00884937 time: 2:51:44\n",
      "[epoch:   3 step: 16750] train loss: 0.00990967 time: 2:52:11\n",
      "[epoch:   3 step: 16800] train loss: 0.0100506 time: 2:52:37\n",
      "[epoch:   3 step: 16850] train loss: 0.00953556 time: 2:53:03\n",
      "[epoch:   3 step: 16900] train loss: 0.0110648 time: 2:53:30\n",
      "[epoch:   3 step: 16950] train loss: 0.010604 time: 2:53:56\n",
      "[epoch:   3 step: 17000] train loss: 0.00935855 time: 2:54:22\n",
      "[epoch:   3 step: 17050] train loss: 0.00947711 time: 2:54:49\n",
      "[epoch:   3 step: 17100] train loss: 0.00941493 time: 2:55:15\n",
      "[epoch:   3 step: 17150] train loss: 0.0100314 time: 2:55:41\n",
      "[epoch:   3 step: 17200] train loss: 0.0104846 time: 2:56:08\n",
      "[epoch:   3 step: 17250] train loss: 0.0121088 time: 2:56:34\n",
      "[epoch:   3 step: 17300] train loss: 0.0114051 time: 2:57:00\n",
      "[epoch:   3 step: 17350] train loss: 0.0100811 time: 2:57:27\n",
      "[epoch:   3 step: 17400] train loss: 0.00924118 time: 2:57:53\n",
      "[epoch:   3 step: 17450] train loss: 0.0107537 time: 2:58:19\n",
      "[epoch:   3 step: 17500] train loss: 0.00940173 time: 2:58:46\n",
      "[epoch:   3 step: 17550] train loss: 0.00925783 time: 2:59:12\n",
      "[epoch:   3 step: 17600] train loss: 0.00933467 time: 2:59:38\n",
      "[epoch:   3 step: 17650] train loss: 0.00951881 time: 3:00:05\n",
      "[epoch:   3 step: 17700] train loss: 0.00905789 time: 3:00:31\n",
      "[epoch:   3 step: 17750] train loss: 0.00936323 time: 3:00:57\n",
      "[epoch:   3 step: 17800] train loss: 0.00971313 time: 3:01:23\n",
      "[epoch:   3 step: 17850] train loss: 0.00946205 time: 3:01:50\n",
      "[epoch:   3 step: 17900] train loss: 0.00867806 time: 3:02:16\n",
      "[epoch:   3 step: 17950] train loss: 0.00912403 time: 3:02:42\n",
      "[epoch:   3 step: 18000] train loss: 0.0101685 time: 3:03:09\n",
      "[epoch:   3 step: 18050] train loss: 0.0104951 time: 3:03:35\n",
      "[epoch:   3 step: 18100] train loss: 0.00981604 time: 3:04:01\n",
      "[epoch:   3 step: 18150] train loss: 0.0109144 time: 3:04:28\n",
      "[epoch:   3 step: 18200] train loss: 0.0127016 time: 3:04:54\n",
      "[epoch:   3 step: 18250] train loss: 0.0110445 time: 3:05:20\n",
      "[epoch:   3 step: 18300] train loss: 0.0101499 time: 3:05:47\n",
      "[epoch:   3 step: 18350] train loss: 0.00963984 time: 3:06:13\n",
      "[epoch:   3 step: 18400] train loss: 0.0111466 time: 3:06:39\n",
      "[epoch:   3 step: 18450] train loss: 0.0101676 time: 3:07:05\n",
      "[epoch:   3 step: 18500] train loss: 0.0098023 time: 3:07:32\n",
      "[epoch:   3 step: 18550] train loss: 0.00942597 time: 3:07:58\n",
      "[epoch:   3 step: 18600] train loss: 0.00860017 time: 3:08:24\n",
      "[epoch:   3 step: 18650] train loss: 0.00844206 time: 3:08:51\n",
      "[epoch:   3 step: 18700] train loss: 0.0102205 time: 3:09:17\n",
      "[epoch:   3 step: 18750] train loss: 0.00895593 time: 3:09:44\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[epoch:   4 step: 18800] train loss: 0.00806938 time: 3:10:10\n",
      "[epoch:   4 step: 18850] train loss: 0.00827525 time: 3:10:37\n",
      "[epoch:   4 step: 18900] train loss: 0.00839871 time: 3:11:03\n",
      "[epoch:   4 step: 18950] train loss: 0.00794444 time: 3:11:30\n",
      "[epoch:   4 step: 19000] train loss: 0.00859046 time: 3:11:56\n",
      "[epoch:   4 step: 19050] train loss: 0.00811513 time: 3:12:22\n",
      "[epoch:   4 step: 19100] train loss: 0.00828339 time: 3:12:49\n",
      "[epoch:   4 step: 19150] train loss: 0.00726947 time: 3:13:15\n",
      "[epoch:   4 step: 19200] train loss: 0.00879682 time: 3:13:41\n",
      "[epoch:   4 step: 19250] train loss: 0.00773391 time: 3:14:08\n",
      "[epoch:   4 step: 19300] train loss: 0.00943831 time: 3:14:34\n",
      "[epoch:   4 step: 19350] train loss: 0.0084897 time: 3:15:00\n",
      "[epoch:   4 step: 19400] train loss: 0.00946802 time: 3:15:27\n",
      "[epoch:   4 step: 19450] train loss: 0.00897509 time: 3:15:53\n",
      "[epoch:   4 step: 19500] train loss: 0.00971851 time: 3:16:20\n",
      "[epoch:   4 step: 19550] train loss: 0.00900341 time: 3:16:46\n",
      "[epoch:   4 step: 19600] train loss: 0.00887973 time: 3:17:12\n",
      "[epoch:   4 step: 19650] train loss: 0.00915484 time: 3:17:39\n",
      "[epoch:   4 step: 19700] train loss: 0.0088317 time: 3:18:05\n",
      "[epoch:   4 step: 19750] train loss: 0.00919497 time: 3:18:32\n",
      "[epoch:   4 step: 19800] train loss: 0.00871397 time: 3:18:58\n",
      "[epoch:   4 step: 19850] train loss: 0.00796352 time: 3:19:24\n",
      "[epoch:   4 step: 19900] train loss: 0.00825339 time: 3:19:51\n",
      "[epoch:   4 step: 19950] train loss: 0.00765873 time: 3:20:17\n",
      "[epoch:   4 step: 20000] train loss: 0.0105132 time: 3:20:44\n",
      "[epoch:   4 step: 20050] train loss: 0.00998733 time: 3:21:10\n",
      "[epoch:   4 step: 20100] train loss: 0.00866274 time: 3:21:37\n",
      "[epoch:   4 step: 20150] train loss: 0.00860653 time: 3:22:03\n",
      "[epoch:   4 step: 20200] train loss: 0.00818418 time: 3:22:30\n",
      "[epoch:   4 step: 20250] train loss: 0.00924515 time: 3:22:56\n",
      "[epoch:   4 step: 20300] train loss: 0.00992812 time: 3:23:22\n",
      "[epoch:   4 step: 20350] train loss: 0.00903613 time: 3:23:49\n",
      "[epoch:   4 step: 20400] train loss: 0.00822753 time: 3:24:15\n",
      "[epoch:   4 step: 20450] train loss: 0.00781568 time: 3:24:42\n",
      "[epoch:   4 step: 20500] train loss: 0.00810599 time: 3:25:08\n",
      "[epoch:   4 step: 20550] train loss: 0.00823018 time: 3:25:35\n",
      "[epoch:   4 step: 20600] train loss: 0.00930147 time: 3:26:01\n",
      "[epoch:   4 step: 20650] train loss: 0.00984867 time: 3:26:28\n",
      "[epoch:   4 step: 20700] train loss: 0.0078726 time: 3:26:54\n",
      "[epoch:   4 step: 20750] train loss: 0.00825985 time: 3:27:20\n",
      "[epoch:   4 step: 20800] train loss: 0.00849332 time: 3:27:47\n",
      "[epoch:   4 step: 20850] train loss: 0.0074884 time: 3:28:13\n",
      "[epoch:   4 step: 20900] train loss: 0.0084079 time: 3:28:39\n",
      "[epoch:   4 step: 20950] train loss: 0.0081532 time: 3:29:06\n",
      "[epoch:   4 step: 21000] train loss: 0.00803188 time: 3:29:32\n",
      "[epoch:   4 step: 21050] train loss: 0.00918136 time: 3:29:59\n",
      "[epoch:   4 step: 21100] train loss: 0.0086438 time: 3:30:25\n",
      "[epoch:   4 step: 21150] train loss: 0.00829016 time: 3:30:51\n",
      "[epoch:   4 step: 21200] train loss: 0.00850465 time: 3:31:18\n",
      "[epoch:   4 step: 21250] train loss: 0.00774348 time: 3:31:44\n",
      "[epoch:   4 step: 21300] train loss: 0.00808862 time: 3:32:11\n",
      "[epoch:   4 step: 21350] train loss: 0.00871644 time: 3:32:37\n",
      "[epoch:   4 step: 21400] train loss: 0.00966947 time: 3:33:04\n",
      "[epoch:   4 step: 21450] train loss: 0.00829837 time: 3:33:30\n",
      "[epoch:   4 step: 21500] train loss: 0.00843462 time: 3:33:56\n",
      "[epoch:   4 step: 21550] train loss: 0.00868335 time: 3:34:23\n",
      "[epoch:   4 step: 21600] train loss: 0.0084091 time: 3:34:49\n",
      "[epoch:   4 step: 21650] train loss: 0.00850298 time: 3:35:15\n",
      "[epoch:   4 step: 21700] train loss: 0.0100877 time: 3:35:42\n",
      "[epoch:   4 step: 21750] train loss: 0.00709349 time: 3:36:08\n",
      "[epoch:   4 step: 21800] train loss: 0.00837286 time: 3:36:35\n",
      "[epoch:   4 step: 21850] train loss: 0.00846879 time: 3:37:01\n",
      "[epoch:   4 step: 21900] train loss: 0.00989651 time: 3:37:28\n",
      "[epoch:   4 step: 21950] train loss: 0.00868856 time: 3:37:54\n",
      "[epoch:   4 step: 22000] train loss: 0.00899964 time: 3:38:21\n",
      "[epoch:   4 step: 22050] train loss: 0.00851849 time: 3:38:47\n",
      "[epoch:   4 step: 22100] train loss: 0.00838019 time: 3:39:14\n",
      "[epoch:   4 step: 22150] train loss: 0.00869047 time: 3:39:40\n",
      "[epoch:   4 step: 22200] train loss: 0.00978885 time: 3:40:06\n",
      "[epoch:   4 step: 22250] train loss: 0.00827999 time: 3:40:33\n",
      "[epoch:   4 step: 22300] train loss: 0.00933277 time: 3:40:59\n",
      "[epoch:   4 step: 22350] train loss: 0.00892882 time: 3:41:26\n",
      "[epoch:   4 step: 22400] train loss: 0.00969379 time: 3:41:52\n",
      "[epoch:   4 step: 22450] train loss: 0.00905541 time: 3:42:18\n",
      "[epoch:   4 step: 22500] train loss: 0.0101817 time: 3:42:45\n",
      "[epoch:   4 step: 22550] train loss: 0.00884648 time: 3:43:11\n",
      "[epoch:   4 step: 22600] train loss: 0.00696166 time: 3:43:38\n",
      "[epoch:   4 step: 22650] train loss: 0.00902336 time: 3:44:04\n",
      "[epoch:   4 step: 22700] train loss: 0.00797487 time: 3:44:31\n",
      "[epoch:   4 step: 22750] train loss: 0.00827129 time: 3:44:57\n",
      "[epoch:   4 step: 22800] train loss: 0.00721425 time: 3:45:23\n",
      "[epoch:   4 step: 22850] train loss: 0.00903245 time: 3:45:50\n",
      "[epoch:   4 step: 22900] train loss: 0.00986198 time: 3:46:16\n",
      "[epoch:   4 step: 22950] train loss: 0.00812002 time: 3:46:43\n",
      "[epoch:   4 step: 23000] train loss: 0.00829377 time: 3:47:09\n",
      "[epoch:   4 step: 23050] train loss: 0.00894395 time: 3:47:35\n",
      "[epoch:   4 step: 23100] train loss: 0.00858846 time: 3:48:02\n",
      "[epoch:   4 step: 23150] train loss: 0.00771042 time: 3:48:28\n",
      "[epoch:   4 step: 23200] train loss: 0.00820498 time: 3:48:55\n",
      "[epoch:   4 step: 23250] train loss: 0.00780279 time: 3:49:21\n",
      "[epoch:   4 step: 23300] train loss: 0.00902193 time: 3:49:47\n",
      "[epoch:   4 step: 23350] train loss: 0.00891222 time: 3:50:14\n",
      "[epoch:   4 step: 23400] train loss: 0.00764997 time: 3:50:40\n",
      "[epoch:   4 step: 23450] train loss: 0.00738854 time: 3:51:07\n",
      "[epoch:   4 step: 23500] train loss: 0.00882823 time: 3:51:33\n",
      "[epoch:   4 step: 23550] train loss: 0.00852321 time: 3:51:59\n",
      "[epoch:   4 step: 23600] train loss: 0.00885464 time: 3:52:26\n",
      "[epoch:   4 step: 23650] train loss: 0.00834292 time: 3:52:52\n",
      "[epoch:   4 step: 23700] train loss: 0.0083822 time: 3:53:18\n",
      "[epoch:   4 step: 23750] train loss: 0.00935322 time: 3:53:45\n",
      "[epoch:   4 step: 23800] train loss: 0.00817626 time: 3:54:11\n",
      "[epoch:   4 step: 23850] train loss: 0.00951542 time: 3:54:38\n",
      "[epoch:   4 step: 23900] train loss: 0.00761866 time: 3:55:04\n",
      "[epoch:   4 step: 23950] train loss: 0.00896536 time: 3:55:31\n",
      "[epoch:   4 step: 24000] train loss: 0.00893653 time: 3:55:57\n",
      "[epoch:   4 step: 24050] train loss: 0.00899674 time: 3:56:24\n",
      "[epoch:   4 step: 24100] train loss: 0.00919059 time: 3:56:50\n",
      "[epoch:   4 step: 24150] train loss: 0.00872271 time: 3:57:16\n",
      "[epoch:   4 step: 24200] train loss: 0.00963768 time: 3:57:43\n",
      "[epoch:   4 step: 24250] train loss: 0.00781904 time: 3:58:09\n",
      "[epoch:   4 step: 24300] train loss: 0.00869619 time: 3:58:36\n",
      "[epoch:   4 step: 24350] train loss: 0.0082783 time: 3:59:02\n",
      "[epoch:   4 step: 24400] train loss: 0.0085903 time: 3:59:28\n",
      "[epoch:   4 step: 24450] train loss: 0.00900163 time: 3:59:55\n",
      "[epoch:   4 step: 24500] train loss: 0.00788224 time: 4:00:21\n",
      "[epoch:   4 step: 24550] train loss: 0.00895577 time: 4:00:47\n",
      "[epoch:   4 step: 24600] train loss: 0.00984822 time: 4:01:14\n",
      "[epoch:   4 step: 24650] train loss: 0.00817056 time: 4:01:40\n",
      "[epoch:   4 step: 24700] train loss: 0.00857866 time: 4:02:07\n",
      "[epoch:   4 step: 24750] train loss: 0.00853123 time: 4:02:33\n",
      "[epoch:   4 step: 24800] train loss: 0.0095453 time: 4:02:59\n",
      "[epoch:   4 step: 24850] train loss: 0.00806302 time: 4:03:26\n",
      "[epoch:   4 step: 24900] train loss: 0.0086151 time: 4:03:52\n",
      "[epoch:   4 step: 24950] train loss: 0.00809155 time: 4:04:19\n",
      "[epoch:   4 step: 25000] train loss: 0.00802652 time: 4:04:45\n"
     ]
    }
   ],
   "source": [
    "!python train_Bleurt.py --model_type Elron/bleurt-base-512  --adapter_name debiased-bleurt --lr 1e-4 --warmup 0.0 --batch_size 16  --n_epochs 4  --seed 42  --device cuda --logging_steps 200  --data_path bleurt.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de952b52-dad7-43c4-b56e-a31f8ee5f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: memory problem when batchsize 32. Machine with 45GB RAM helps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
