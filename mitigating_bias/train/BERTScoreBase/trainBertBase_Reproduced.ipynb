{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa173ff-b7a8-4819-91bb-4c8915239bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: adapter_transformers==3.1.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: fastNLP==0.7.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: fitlog==0.9.13 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 3)) (0.9.13)\n",
      "Collecting matplotlib==3.4.3\n",
      "  Downloading matplotlib-3.4.3-cp39-cp39-manylinux1_x86_64.whl (10.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy==1.20.3 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 5)) (1.20.3)\n",
      "Collecting pandas==1.3.4\n",
      "  Downloading pandas-1.3.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hCollecting sacrebleu==2.2.1\n",
      "  Downloading sacrebleu-2.2.1-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy==1.7.1\n",
      "  Downloading scipy-1.7.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.5/28.5 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting setuptools==58.0.4\n",
      "  Downloading setuptools-58.0.4-py3-none-any.whl (816 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m816.5/816.5 kB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm==4.62.3\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: transformers==4.23.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 11)) (4.23.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (0.11.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (3.7.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (2022.7.9)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (2.28.1)\n",
      "Requirement already satisfied: prettytable>=0.7.2 in /usr/local/lib/python3.9/dist-packages (from fastNLP==0.7.0->-r requirements.txt (line 2)) (3.6.0)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from fastNLP==0.7.0->-r requirements.txt (line 2)) (1.12.0+cu116)\n",
      "Requirement already satisfied: gitpython>=3.1.2 in /usr/local/lib/python3.9/dist-packages (from fitlog==0.9.13->-r requirements.txt (line 3)) (3.1.27)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.9/dist-packages (from fitlog==0.9.13->-r requirements.txt (line 3)) (0.6.2)\n",
      "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.9/dist-packages (from fitlog==0.9.13->-r requirements.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.4.3->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.4.3->-r requirements.txt (line 4)) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.4.3->-r requirements.txt (line 4)) (1.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.4.3->-r requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.4.3->-r requirements.txt (line 4)) (9.2.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas==1.3.4->-r requirements.txt (line 6)) (2022.1)\n",
      "Requirement already satisfied: lxml in /usr/lib/python3/dist-packages (from sacrebleu==2.2.1->-r requirements.txt (line 7)) (4.5.0)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: colorama in /usr/lib/python3/dist-packages (from sacrebleu==2.2.1->-r requirements.txt (line 7)) (0.4.3)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (4.12.0)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.9/dist-packages (from flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.9/dist-packages (from flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.9/dist-packages (from flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (8.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.9/dist-packages (from flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (2.1.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from gitpython>=3.1.2->fitlog==0.9.13->-r requirements.txt (line 3)) (4.0.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (4.3.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prettytable>=0.7.2->fastNLP==0.7.0->-r requirements.txt (line 2)) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib==3.4.3->-r requirements.txt (line 4)) (1.14.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (1.26.10)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.2->fitlog==0.9.13->-r requirements.txt (line 3)) (5.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6.0->flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (3.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from Jinja2>=3.0->flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (2.1.1)\n",
      "Installing collected packages: tqdm, tabulate, setuptools, scipy, portalocker, sacrebleu, pandas, matplotlib\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.0\n",
      "    Uninstalling tqdm-4.64.0:\n",
      "      Successfully uninstalled tqdm-4.64.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 63.1.0\n",
      "    Uninstalling setuptools-63.1.0:\n",
      "      Successfully uninstalled setuptools-63.1.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.8.1\n",
      "    Uninstalling scipy-1.8.1:\n",
      "      Successfully uninstalled scipy-1.8.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.4.3\n",
      "    Uninstalling pandas-1.4.3:\n",
      "      Successfully uninstalled pandas-1.4.3\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.5.2\n",
      "    Uninstalling matplotlib-3.5.2:\n",
      "      Successfully uninstalled matplotlib-3.5.2\n",
      "Successfully installed matplotlib-3.4.3 pandas-1.3.4 portalocker-2.6.0 sacrebleu-2.2.1 scipy-1.7.1 setuptools-58.0.4 tabulate-0.9.0 tqdm-4.62.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfd2660a-4ab6-4f52-80cc-447ed08e4a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.12.0+cu116)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.13.0+cu116)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (0.12.0+cu116)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.20.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.10)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "315ba16f-1c65-4acc-a5e8-91512792d5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|█████| 28.0/28.0 [00:00<00:00, 30.8kB/s]\n",
      "Downloading config.json: 100%|██████████████████| 570/570 [00:00<00:00, 757kB/s]\n",
      "Downloading vocab.txt: 100%|█████████████████| 226k/226k [00:00<00:00, 6.06MB/s]\n",
      "Downloading pytorch_model.bin: 100%|█████████| 420M/420M [00:04<00:00, 91.9MB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Read cache from cached_data.bin.\n",
      "# samples: 100000\n",
      "Example:\n",
      "+----------------+----------------+----------------+--------+\n",
      "| refs           | hyps           | labels         | type   |\n",
      "+----------------+----------------+----------------+--------+\n",
      "| a man is pl... | a person is... | 0.902562737... | debias |\n",
      "|                |                |                |        |\n",
      "+----------------+----------------+----------------+--------+\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "input fields after batch(if batch size is 2):\n",
      "\trefs: (1)type:numpy.ndarray (2)dtype:<U32, (3)shape:(2,) \n",
      "\thyps: (1)type:numpy.ndarray (2)dtype:<U33, (3)shape:(2,) \n",
      "\tlabels: (1)type:torch.Tensor (2)dtype:torch.float32, (3)shape:torch.Size([2]) \n",
      "target fields after batch(if batch size is 2):\n",
      "\tlabels: (1)type:torch.Tensor (2)dtype:torch.float32, (3)shape:torch.Size([2]) \n",
      "\n",
      "training epochs started 2023-01-12-07-18-57-411276\n",
      "[epoch:   1 step:   50] train loss: 0.00484036 time: 0:00:06\n",
      "[epoch:   1 step:  100] train loss: 0.00201127 time: 0:00:12\n",
      "[epoch:   1 step:  150] train loss: 0.00189899 time: 0:00:18\n",
      "[epoch:   1 step:  200] train loss: 0.00154945 time: 0:00:24\n",
      "[epoch:   1 step:  250] train loss: 0.00142962 time: 0:00:30\n",
      "[epoch:   1 step:  300] train loss: 0.00129902 time: 0:00:36\n",
      "[epoch:   1 step:  350] train loss: 0.00141384 time: 0:00:42\n",
      "[epoch:   1 step:  400] train loss: 0.00121965 time: 0:00:48\n",
      "[epoch:   1 step:  450] train loss: 0.00109981 time: 0:00:54\n",
      "[epoch:   1 step:  500] train loss: 0.00105653 time: 0:00:59\n",
      "[epoch:   1 step:  550] train loss: 0.000971922 time: 0:01:05\n",
      "[epoch:   1 step:  600] train loss: 0.000873451 time: 0:01:11\n",
      "[epoch:   1 step:  650] train loss: 0.00102305 time: 0:01:17\n",
      "[epoch:   1 step:  700] train loss: 0.00115596 time: 0:01:23\n",
      "[epoch:   1 step:  750] train loss: 0.000801255 time: 0:01:29\n",
      "[epoch:   1 step:  800] train loss: 0.000885486 time: 0:01:35\n",
      "[epoch:   1 step:  850] train loss: 0.000813249 time: 0:01:40\n",
      "[epoch:   1 step:  900] train loss: 0.000732793 time: 0:01:46\n",
      "[epoch:   1 step:  950] train loss: 0.000774715 time: 0:01:52\n",
      "[epoch:   1 step: 1000] train loss: 0.00075593 time: 0:01:58\n",
      "[epoch:   1 step: 1050] train loss: 0.000719348 time: 0:02:03\n",
      "[epoch:   1 step: 1100] train loss: 0.000748645 time: 0:02:09\n",
      "[epoch:   1 step: 1150] train loss: 0.000708485 time: 0:02:15\n",
      "[epoch:   1 step: 1200] train loss: 0.000716454 time: 0:02:21\n",
      "[epoch:   1 step: 1250] train loss: 0.000705924 time: 0:02:27\n",
      "[epoch:   1 step: 1300] train loss: 0.000666532 time: 0:02:32\n",
      "[epoch:   1 step: 1350] train loss: 0.000669243 time: 0:02:38\n",
      "[epoch:   1 step: 1400] train loss: 0.00075219 time: 0:02:44\n",
      "[epoch:   1 step: 1450] train loss: 0.000742501 time: 0:02:50\n",
      "[epoch:   1 step: 1500] train loss: 0.000734824 time: 0:02:56\n",
      "[epoch:   1 step: 1550] train loss: 0.000613644 time: 0:03:01\n",
      "[epoch:   1 step: 1600] train loss: 0.000669274 time: 0:03:07\n",
      "[epoch:   1 step: 1650] train loss: 0.000635119 time: 0:03:13\n",
      "[epoch:   1 step: 1700] train loss: 0.000642246 time: 0:03:19\n",
      "[epoch:   1 step: 1750] train loss: 0.000656563 time: 0:03:25\n",
      "[epoch:   1 step: 1800] train loss: 0.000582697 time: 0:03:30\n",
      "[epoch:   1 step: 1850] train loss: 0.000661921 time: 0:03:36\n",
      "[epoch:   1 step: 1900] train loss: 0.000623881 time: 0:03:42\n",
      "[epoch:   1 step: 1950] train loss: 0.000617916 time: 0:03:47\n",
      "[epoch:   1 step: 2000] train loss: 0.000645065 time: 0:03:53\n",
      "[epoch:   1 step: 2050] train loss: 0.000534323 time: 0:03:59\n",
      "[epoch:   1 step: 2100] train loss: 0.000515173 time: 0:04:05\n",
      "[epoch:   1 step: 2150] train loss: 0.000625641 time: 0:04:10\n",
      "[epoch:   1 step: 2200] train loss: 0.000621268 time: 0:04:16\n",
      "[epoch:   1 step: 2250] train loss: 0.000557575 time: 0:04:22\n",
      "[epoch:   1 step: 2300] train loss: 0.00054409 time: 0:04:28\n",
      "[epoch:   1 step: 2350] train loss: 0.000640829 time: 0:04:34\n",
      "[epoch:   1 step: 2400] train loss: 0.000613644 time: 0:04:40\n",
      "[epoch:   1 step: 2450] train loss: 0.000556277 time: 0:04:46\n",
      "[epoch:   1 step: 2500] train loss: 0.000531066 time: 0:04:52\n",
      "[epoch:   1 step: 2550] train loss: 0.0005843 time: 0:04:58\n",
      "[epoch:   1 step: 2600] train loss: 0.000578442 time: 0:05:03\n",
      "[epoch:   1 step: 2650] train loss: 0.000552323 time: 0:05:09\n",
      "[epoch:   1 step: 2700] train loss: 0.000481622 time: 0:05:15\n",
      "[epoch:   1 step: 2750] train loss: 0.000544994 time: 0:05:20\n",
      "[epoch:   1 step: 2800] train loss: 0.00058079 time: 0:05:26\n",
      "[epoch:   1 step: 2850] train loss: 0.000531863 time: 0:05:32\n",
      "[epoch:   1 step: 2900] train loss: 0.000453249 time: 0:05:38\n",
      "[epoch:   1 step: 2950] train loss: 0.000513432 time: 0:05:43\n",
      "[epoch:   1 step: 3000] train loss: 0.000455537 time: 0:05:49\n",
      "[epoch:   1 step: 3050] train loss: 0.000521431 time: 0:05:55\n",
      "[epoch:   1 step: 3100] train loss: 0.000548603 time: 0:06:01\n",
      "[epoch:   1 step: 3150] train loss: 0.000574526 time: 0:06:06\n",
      "[epoch:   1 step: 3200] train loss: 0.000554435 time: 0:06:12\n",
      "[epoch:   1 step: 3250] train loss: 0.000576591 time: 0:06:18\n",
      "[epoch:   1 step: 3300] train loss: 0.000527482 time: 0:06:24\n",
      "[epoch:   1 step: 3350] train loss: 0.000554319 time: 0:06:30\n",
      "[epoch:   1 step: 3400] train loss: 0.000560213 time: 0:06:36\n",
      "[epoch:   1 step: 3450] train loss: 0.000513301 time: 0:06:41\n",
      "[epoch:   1 step: 3500] train loss: 0.00047821 time: 0:06:47\n",
      "[epoch:   1 step: 3550] train loss: 0.000469807 time: 0:06:53\n",
      "[epoch:   1 step: 3600] train loss: 0.000528107 time: 0:06:59\n",
      "[epoch:   1 step: 3650] train loss: 0.000514606 time: 0:07:04\n",
      "[epoch:   1 step: 3700] train loss: 0.000470234 time: 0:07:10\n",
      "[epoch:   1 step: 3750] train loss: 0.000476948 time: 0:07:16\n",
      "[epoch:   1 step: 3800] train loss: 0.000559003 time: 0:07:22\n",
      "[epoch:   1 step: 3850] train loss: 0.000480737 time: 0:07:28\n",
      "[epoch:   1 step: 3900] train loss: 0.000441158 time: 0:07:33\n",
      "[epoch:   1 step: 3950] train loss: 0.000522941 time: 0:07:39\n",
      "[epoch:   1 step: 4000] train loss: 0.000486838 time: 0:07:45\n",
      "[epoch:   1 step: 4050] train loss: 0.000493963 time: 0:07:51\n",
      "[epoch:   1 step: 4100] train loss: 0.000463901 time: 0:07:56\n",
      "[epoch:   1 step: 4150] train loss: 0.000494307 time: 0:08:02\n",
      "[epoch:   1 step: 4200] train loss: 0.00046582 time: 0:08:08\n",
      "[epoch:   1 step: 4250] train loss: 0.000450436 time: 0:08:13\n",
      "[epoch:   1 step: 4300] train loss: 0.000505959 time: 0:08:19\n",
      "[epoch:   1 step: 4350] train loss: 0.000521043 time: 0:08:24\n",
      "[epoch:   1 step: 4400] train loss: 0.000441689 time: 0:08:30\n",
      "[epoch:   1 step: 4450] train loss: 0.00048837 time: 0:08:35\n",
      "[epoch:   1 step: 4500] train loss: 0.000433355 time: 0:08:41\n",
      "[epoch:   1 step: 4550] train loss: 0.000414319 time: 0:08:46\n",
      "[epoch:   1 step: 4600] train loss: 0.00050909 time: 0:08:52\n",
      "[epoch:   1 step: 4650] train loss: 0.00056044 time: 0:08:58\n",
      "[epoch:   1 step: 4700] train loss: 0.000500966 time: 0:09:04\n",
      "[epoch:   1 step: 4750] train loss: 0.000499254 time: 0:09:10\n",
      "[epoch:   1 step: 4800] train loss: 0.000480761 time: 0:09:15\n",
      "[epoch:   1 step: 4850] train loss: 0.000577442 time: 0:09:21\n",
      "[epoch:   1 step: 4900] train loss: 0.000473179 time: 0:09:26\n",
      "[epoch:   1 step: 4950] train loss: 0.000429318 time: 0:09:32\n",
      "[epoch:   1 step: 5000] train loss: 0.000412545 time: 0:09:38\n",
      "[epoch:   1 step: 5050] train loss: 0.000410076 time: 0:09:43\n",
      "[epoch:   1 step: 5100] train loss: 0.000481565 time: 0:09:49\n",
      "[epoch:   1 step: 5150] train loss: 0.000384906 time: 0:09:55\n",
      "[epoch:   1 step: 5200] train loss: 0.000494097 time: 0:10:01\n",
      "[epoch:   1 step: 5250] train loss: 0.000495302 time: 0:10:06\n",
      "[epoch:   1 step: 5300] train loss: 0.00047919 time: 0:10:12\n",
      "[epoch:   1 step: 5350] train loss: 0.000432605 time: 0:10:18\n",
      "[epoch:   1 step: 5400] train loss: 0.000503864 time: 0:10:23\n",
      "[epoch:   1 step: 5450] train loss: 0.000426067 time: 0:10:29\n",
      "[epoch:   1 step: 5500] train loss: 0.000511468 time: 0:10:35\n",
      "[epoch:   1 step: 5550] train loss: 0.000518949 time: 0:10:41\n",
      "[epoch:   1 step: 5600] train loss: 0.000398371 time: 0:10:46\n",
      "[epoch:   1 step: 5650] train loss: 0.00040596 time: 0:10:52\n",
      "[epoch:   1 step: 5700] train loss: 0.00048928 time: 0:10:58\n",
      "[epoch:   1 step: 5750] train loss: 0.000515541 time: 0:11:04\n",
      "[epoch:   1 step: 5800] train loss: 0.000466743 time: 0:11:10\n",
      "[epoch:   1 step: 5850] train loss: 0.000473228 time: 0:11:15\n",
      "[epoch:   1 step: 5900] train loss: 0.000411817 time: 0:11:21\n",
      "[epoch:   1 step: 5950] train loss: 0.000468488 time: 0:11:26\n",
      "[epoch:   1 step: 6000] train loss: 0.000391932 time: 0:11:32\n",
      "[epoch:   1 step: 6050] train loss: 0.000476677 time: 0:11:38\n",
      "[epoch:   1 step: 6100] train loss: 0.000448865 time: 0:11:44\n",
      "[epoch:   1 step: 6150] train loss: 0.000337394 time: 0:11:50\n",
      "[epoch:   1 step: 6200] train loss: 0.00039796 time: 0:11:55\n",
      "[epoch:   1 step: 6250] train loss: 0.000401829 time: 0:12:01\n",
      "[epoch:   2 step: 6300] train loss: 0.000370595 time: 0:12:07\n",
      "[epoch:   2 step: 6350] train loss: 0.000358621 time: 0:12:12\n",
      "[epoch:   2 step: 6400] train loss: 0.000348489 time: 0:12:18\n",
      "[epoch:   2 step: 6450] train loss: 0.000373986 time: 0:12:24\n",
      "[epoch:   2 step: 6500] train loss: 0.000372512 time: 0:12:30\n",
      "[epoch:   2 step: 6550] train loss: 0.000391675 time: 0:12:36\n",
      "[epoch:   2 step: 6600] train loss: 0.000411998 time: 0:12:41\n",
      "[epoch:   2 step: 6650] train loss: 0.000383411 time: 0:12:47\n",
      "[epoch:   2 step: 6700] train loss: 0.00035208 time: 0:12:53\n",
      "[epoch:   2 step: 6750] train loss: 0.00034592 time: 0:12:58\n",
      "[epoch:   2 step: 6800] train loss: 0.000368116 time: 0:13:04\n",
      "[epoch:   2 step: 6850] train loss: 0.000532658 time: 0:13:10\n",
      "[epoch:   2 step: 6900] train loss: 0.000476641 time: 0:13:16\n",
      "[epoch:   2 step: 6950] train loss: 0.000444227 time: 0:13:21\n",
      "[epoch:   2 step: 7000] train loss: 0.000355452 time: 0:13:27\n",
      "[epoch:   2 step: 7050] train loss: 0.000343812 time: 0:13:33\n",
      "[epoch:   2 step: 7100] train loss: 0.000408147 time: 0:13:39\n",
      "[epoch:   2 step: 7150] train loss: 0.000405816 time: 0:13:45\n",
      "[epoch:   2 step: 7200] train loss: 0.000378772 time: 0:13:51\n",
      "[epoch:   2 step: 7250] train loss: 0.000415944 time: 0:13:56\n",
      "[epoch:   2 step: 7300] train loss: 0.000367323 time: 0:14:02\n",
      "[epoch:   2 step: 7350] train loss: 0.000372742 time: 0:14:08\n",
      "[epoch:   2 step: 7400] train loss: 0.000356783 time: 0:14:14\n",
      "[epoch:   2 step: 7450] train loss: 0.000350181 time: 0:14:19\n",
      "[epoch:   2 step: 7500] train loss: 0.000370091 time: 0:14:25\n",
      "[epoch:   2 step: 7550] train loss: 0.000359794 time: 0:14:30\n",
      "[epoch:   2 step: 7600] train loss: 0.000384761 time: 0:14:36\n",
      "[epoch:   2 step: 7650] train loss: 0.000353844 time: 0:14:42\n",
      "[epoch:   2 step: 7700] train loss: 0.000352721 time: 0:14:47\n",
      "[epoch:   2 step: 7750] train loss: 0.00037028 time: 0:14:53\n",
      "[epoch:   2 step: 7800] train loss: 0.000339969 time: 0:14:59\n",
      "[epoch:   2 step: 7850] train loss: 0.000382928 time: 0:15:05\n",
      "[epoch:   2 step: 7900] train loss: 0.000361206 time: 0:15:11\n",
      "[epoch:   2 step: 7950] train loss: 0.000360168 time: 0:15:16\n",
      "[epoch:   2 step: 8000] train loss: 0.000364521 time: 0:15:22\n",
      "[epoch:   2 step: 8050] train loss: 0.000346366 time: 0:15:28\n",
      "[epoch:   2 step: 8100] train loss: 0.00036232 time: 0:15:34\n",
      "[epoch:   2 step: 8150] train loss: 0.000347658 time: 0:15:39\n",
      "[epoch:   2 step: 8200] train loss: 0.000344034 time: 0:15:45\n",
      "[epoch:   2 step: 8250] train loss: 0.000368635 time: 0:15:51\n",
      "[epoch:   2 step: 8300] train loss: 0.000368359 time: 0:15:57\n",
      "[epoch:   2 step: 8350] train loss: 0.000349583 time: 0:16:02\n",
      "[epoch:   2 step: 8400] train loss: 0.000347652 time: 0:16:08\n",
      "[epoch:   2 step: 8450] train loss: 0.000378165 time: 0:16:14\n",
      "[epoch:   2 step: 8500] train loss: 0.000344873 time: 0:16:20\n",
      "[epoch:   2 step: 8550] train loss: 0.000322852 time: 0:16:25\n",
      "[epoch:   2 step: 8600] train loss: 0.00031474 time: 0:16:32\n",
      "[epoch:   2 step: 8650] train loss: 0.000313469 time: 0:16:37\n",
      "[epoch:   2 step: 8700] train loss: 0.000355521 time: 0:16:43\n",
      "[epoch:   2 step: 8750] train loss: 0.000330348 time: 0:16:49\n",
      "[epoch:   2 step: 8800] train loss: 0.000361126 time: 0:16:55\n",
      "[epoch:   2 step: 8850] train loss: 0.000376013 time: 0:17:00\n",
      "[epoch:   2 step: 8900] train loss: 0.000351553 time: 0:17:06\n",
      "[epoch:   2 step: 8950] train loss: 0.000395123 time: 0:17:12\n",
      "[epoch:   2 step: 9000] train loss: 0.000373887 time: 0:17:17\n",
      "[epoch:   2 step: 9050] train loss: 0.000328732 time: 0:17:23\n",
      "[epoch:   2 step: 9100] train loss: 0.000406487 time: 0:17:29\n",
      "[epoch:   2 step: 9150] train loss: 0.000377657 time: 0:17:35\n",
      "[epoch:   2 step: 9200] train loss: 0.000386096 time: 0:17:41\n",
      "[epoch:   2 step: 9250] train loss: 0.000355598 time: 0:17:46\n",
      "[epoch:   2 step: 9300] train loss: 0.000348036 time: 0:17:52\n",
      "[epoch:   2 step: 9350] train loss: 0.000404037 time: 0:17:58\n",
      "[epoch:   2 step: 9400] train loss: 0.000359874 time: 0:18:04\n",
      "[epoch:   2 step: 9450] train loss: 0.000345255 time: 0:18:10\n",
      "[epoch:   2 step: 9500] train loss: 0.000352967 time: 0:18:16\n",
      "[epoch:   2 step: 9550] train loss: 0.000337685 time: 0:18:21\n",
      "[epoch:   2 step: 9600] train loss: 0.000316385 time: 0:18:27\n",
      "[epoch:   2 step: 9650] train loss: 0.00040489 time: 0:18:33\n",
      "[epoch:   2 step: 9700] train loss: 0.00041142 time: 0:18:39\n",
      "[epoch:   2 step: 9750] train loss: 0.000354208 time: 0:18:44\n",
      "[epoch:   2 step: 9800] train loss: 0.000367846 time: 0:18:50\n",
      "[epoch:   2 step: 9850] train loss: 0.000337979 time: 0:18:56\n",
      "[epoch:   2 step: 9900] train loss: 0.000383616 time: 0:19:02\n",
      "[epoch:   2 step: 9950] train loss: 0.000381587 time: 0:19:07\n",
      "[epoch:   2 step: 10000] train loss: 0.000352389 time: 0:19:13\n",
      "[epoch:   2 step: 10050] train loss: 0.000327274 time: 0:19:19\n",
      "[epoch:   2 step: 10100] train loss: 0.000409549 time: 0:19:24\n",
      "[epoch:   2 step: 10150] train loss: 0.000354253 time: 0:19:30\n",
      "[epoch:   2 step: 10200] train loss: 0.000372406 time: 0:19:36\n",
      "[epoch:   2 step: 10250] train loss: 0.000387391 time: 0:19:42\n",
      "[epoch:   2 step: 10300] train loss: 0.000349035 time: 0:19:48\n",
      "[epoch:   2 step: 10350] train loss: 0.000331034 time: 0:19:53\n",
      "[epoch:   2 step: 10400] train loss: 0.000400402 time: 0:19:59\n",
      "[epoch:   2 step: 10450] train loss: 0.0003336 time: 0:20:05\n",
      "[epoch:   2 step: 10500] train loss: 0.000387089 time: 0:20:11\n",
      "[epoch:   2 step: 10550] train loss: 0.000344241 time: 0:20:16\n",
      "[epoch:   2 step: 10600] train loss: 0.000426375 time: 0:20:22\n",
      "[epoch:   2 step: 10650] train loss: 0.000330266 time: 0:20:27\n",
      "[epoch:   2 step: 10700] train loss: 0.000368224 time: 0:20:33\n",
      "[epoch:   2 step: 10750] train loss: 0.000419715 time: 0:20:39\n",
      "[epoch:   2 step: 10800] train loss: 0.000423505 time: 0:20:45\n",
      "[epoch:   2 step: 10850] train loss: 0.000342183 time: 0:20:50\n",
      "[epoch:   2 step: 10900] train loss: 0.000356888 time: 0:20:56\n",
      "[epoch:   2 step: 10950] train loss: 0.000341485 time: 0:21:02\n",
      "[epoch:   2 step: 11000] train loss: 0.000374442 time: 0:21:08\n",
      "[epoch:   2 step: 11050] train loss: 0.000349937 time: 0:21:13\n",
      "[epoch:   2 step: 11100] train loss: 0.000367219 time: 0:21:19\n",
      "[epoch:   2 step: 11150] train loss: 0.000335413 time: 0:21:24\n",
      "[epoch:   2 step: 11200] train loss: 0.000328162 time: 0:21:30\n",
      "[epoch:   2 step: 11250] train loss: 0.00034841 time: 0:21:36\n",
      "[epoch:   2 step: 11300] train loss: 0.000340215 time: 0:21:41\n",
      "[epoch:   2 step: 11350] train loss: 0.000331551 time: 0:21:47\n",
      "[epoch:   2 step: 11400] train loss: 0.00036565 time: 0:21:53\n",
      "[epoch:   2 step: 11450] train loss: 0.000354213 time: 0:21:58\n",
      "[epoch:   2 step: 11500] train loss: 0.000397592 time: 0:22:04\n",
      "[epoch:   2 step: 11550] train loss: 0.000304939 time: 0:22:10\n",
      "[epoch:   2 step: 11600] train loss: 0.000369991 time: 0:22:16\n",
      "[epoch:   2 step: 11650] train loss: 0.000354414 time: 0:22:21\n",
      "[epoch:   2 step: 11700] train loss: 0.000362658 time: 0:22:27\n",
      "[epoch:   2 step: 11750] train loss: 0.000376639 time: 0:22:33\n",
      "[epoch:   2 step: 11800] train loss: 0.000338569 time: 0:22:39\n",
      "[epoch:   2 step: 11850] train loss: 0.000384749 time: 0:22:45\n",
      "[epoch:   2 step: 11900] train loss: 0.000347837 time: 0:22:50\n",
      "[epoch:   2 step: 11950] train loss: 0.000405249 time: 0:22:56\n",
      "[epoch:   2 step: 12000] train loss: 0.000349952 time: 0:23:02\n",
      "[epoch:   2 step: 12050] train loss: 0.000368381 time: 0:23:07\n",
      "[epoch:   2 step: 12100] train loss: 0.000322127 time: 0:23:13\n",
      "[epoch:   2 step: 12150] train loss: 0.000368686 time: 0:23:19\n",
      "[epoch:   2 step: 12200] train loss: 0.000380073 time: 0:23:25\n",
      "[epoch:   2 step: 12250] train loss: 0.000361349 time: 0:23:30\n",
      "[epoch:   2 step: 12300] train loss: 0.000380207 time: 0:23:36\n",
      "[epoch:   2 step: 12350] train loss: 0.000345912 time: 0:23:42\n",
      "[epoch:   2 step: 12400] train loss: 0.00029713 time: 0:23:48\n",
      "[epoch:   2 step: 12450] train loss: 0.000314702 time: 0:23:54\n",
      "[epoch:   2 step: 12500] train loss: 0.000321417 time: 0:24:00\n",
      "[epoch:   3 step: 12550] train loss: 0.000310591 time: 0:24:06\n",
      "[epoch:   3 step: 12600] train loss: 0.000306158 time: 0:24:12\n",
      "[epoch:   3 step: 12650] train loss: 0.000254819 time: 0:24:18\n",
      "[epoch:   3 step: 12700] train loss: 0.000248547 time: 0:24:24\n",
      "[epoch:   3 step: 12750] train loss: 0.000256148 time: 0:24:29\n",
      "[epoch:   3 step: 12800] train loss: 0.000241964 time: 0:24:35\n",
      "[epoch:   3 step: 12850] train loss: 0.000279625 time: 0:24:41\n",
      "[epoch:   3 step: 12900] train loss: 0.000290283 time: 0:24:46\n",
      "[epoch:   3 step: 12950] train loss: 0.000315306 time: 0:24:52\n",
      "[epoch:   3 step: 13000] train loss: 0.000283228 time: 0:24:58\n",
      "[epoch:   3 step: 13050] train loss: 0.000291551 time: 0:25:04\n",
      "[epoch:   3 step: 13100] train loss: 0.000274931 time: 0:25:09\n",
      "[epoch:   3 step: 13150] train loss: 0.000281967 time: 0:25:15\n",
      "[epoch:   3 step: 13200] train loss: 0.000267341 time: 0:25:21\n",
      "[epoch:   3 step: 13250] train loss: 0.000284424 time: 0:25:27\n",
      "[epoch:   3 step: 13300] train loss: 0.000314839 time: 0:25:32\n",
      "[epoch:   3 step: 13350] train loss: 0.000376525 time: 0:25:38\n",
      "[epoch:   3 step: 13400] train loss: 0.000355402 time: 0:25:43\n",
      "[epoch:   3 step: 13450] train loss: 0.000326299 time: 0:25:49\n",
      "[epoch:   3 step: 13500] train loss: 0.000291434 time: 0:25:55\n",
      "[epoch:   3 step: 13550] train loss: 0.000320339 time: 0:26:00\n",
      "[epoch:   3 step: 13600] train loss: 0.000286058 time: 0:26:06\n",
      "[epoch:   3 step: 13650] train loss: 0.000299834 time: 0:26:12\n",
      "[epoch:   3 step: 13700] train loss: 0.000345001 time: 0:26:18\n",
      "[epoch:   3 step: 13750] train loss: 0.000275905 time: 0:26:24\n",
      "[epoch:   3 step: 13800] train loss: 0.000294624 time: 0:26:29\n",
      "[epoch:   3 step: 13850] train loss: 0.000278741 time: 0:26:35\n",
      "[epoch:   3 step: 13900] train loss: 0.00030761 time: 0:26:41\n",
      "[epoch:   3 step: 13950] train loss: 0.000323345 time: 0:26:47\n",
      "[epoch:   3 step: 14000] train loss: 0.000290394 time: 0:26:53\n",
      "[epoch:   3 step: 14050] train loss: 0.000299174 time: 0:26:58\n",
      "[epoch:   3 step: 14100] train loss: 0.000320937 time: 0:27:04\n",
      "[epoch:   3 step: 14150] train loss: 0.000291867 time: 0:27:10\n",
      "[epoch:   3 step: 14200] train loss: 0.000318844 time: 0:27:16\n",
      "[epoch:   3 step: 14250] train loss: 0.000267251 time: 0:27:21\n",
      "[epoch:   3 step: 14300] train loss: 0.000299794 time: 0:27:27\n",
      "[epoch:   3 step: 14350] train loss: 0.000301129 time: 0:27:33\n",
      "[epoch:   3 step: 14400] train loss: 0.000327902 time: 0:27:39\n",
      "[epoch:   3 step: 14450] train loss: 0.000307166 time: 0:27:44\n",
      "[epoch:   3 step: 14500] train loss: 0.000327752 time: 0:27:50\n",
      "[epoch:   3 step: 14550] train loss: 0.000300578 time: 0:27:55\n",
      "[epoch:   3 step: 14600] train loss: 0.000277273 time: 0:28:01\n",
      "[epoch:   3 step: 14650] train loss: 0.000275227 time: 0:28:07\n",
      "[epoch:   3 step: 14700] train loss: 0.000280988 time: 0:28:12\n",
      "[epoch:   3 step: 14750] train loss: 0.000253028 time: 0:28:18\n",
      "[epoch:   3 step: 14800] train loss: 0.000258691 time: 0:28:24\n",
      "[epoch:   3 step: 14850] train loss: 0.000271011 time: 0:28:29\n",
      "[epoch:   3 step: 14900] train loss: 0.000264118 time: 0:28:35\n",
      "[epoch:   3 step: 14950] train loss: 0.000283705 time: 0:28:41\n",
      "[epoch:   3 step: 15000] train loss: 0.000286801 time: 0:28:47\n",
      "[epoch:   3 step: 15050] train loss: 0.000303491 time: 0:28:52\n",
      "[epoch:   3 step: 15100] train loss: 0.000293726 time: 0:28:58\n",
      "[epoch:   3 step: 15150] train loss: 0.000301191 time: 0:29:04\n",
      "[epoch:   3 step: 15200] train loss: 0.000276149 time: 0:29:09\n",
      "[epoch:   3 step: 15250] train loss: 0.000338873 time: 0:29:15\n",
      "[epoch:   3 step: 15300] train loss: 0.000327619 time: 0:29:21\n",
      "[epoch:   3 step: 15350] train loss: 0.000312227 time: 0:29:27\n",
      "[epoch:   3 step: 15400] train loss: 0.000279652 time: 0:29:32\n",
      "[epoch:   3 step: 15450] train loss: 0.000253463 time: 0:29:38\n",
      "[epoch:   3 step: 15500] train loss: 0.000302347 time: 0:29:44\n",
      "[epoch:   3 step: 15550] train loss: 0.000285266 time: 0:29:50\n",
      "[epoch:   3 step: 15600] train loss: 0.000297344 time: 0:29:56\n",
      "[epoch:   3 step: 15650] train loss: 0.000268107 time: 0:30:01\n",
      "[epoch:   3 step: 15700] train loss: 0.000301847 time: 0:30:08\n",
      "[epoch:   3 step: 15750] train loss: 0.000288214 time: 0:30:13\n",
      "[epoch:   3 step: 15800] train loss: 0.000277049 time: 0:30:19\n",
      "[epoch:   3 step: 15850] train loss: 0.000310212 time: 0:30:25\n",
      "[epoch:   3 step: 15900] train loss: 0.000281989 time: 0:30:31\n",
      "[epoch:   3 step: 15950] train loss: 0.00029549 time: 0:30:36\n",
      "[epoch:   3 step: 16000] train loss: 0.000269076 time: 0:30:42\n",
      "[epoch:   3 step: 16050] train loss: 0.000320274 time: 0:30:48\n",
      "[epoch:   3 step: 16100] train loss: 0.00029855 time: 0:30:54\n",
      "[epoch:   3 step: 16150] train loss: 0.000334121 time: 0:30:59\n",
      "[epoch:   3 step: 16200] train loss: 0.000293111 time: 0:31:05\n",
      "[epoch:   3 step: 16250] train loss: 0.000270699 time: 0:31:10\n",
      "[epoch:   3 step: 16300] train loss: 0.000298912 time: 0:31:16\n",
      "[epoch:   3 step: 16350] train loss: 0.000381843 time: 0:31:22\n",
      "[epoch:   3 step: 16400] train loss: 0.000291258 time: 0:31:28\n",
      "[epoch:   3 step: 16450] train loss: 0.000309484 time: 0:31:33\n",
      "[epoch:   3 step: 16500] train loss: 0.000300973 time: 0:31:39\n",
      "[epoch:   3 step: 16550] train loss: 0.000276142 time: 0:31:44\n",
      "[epoch:   3 step: 16600] train loss: 0.000300738 time: 0:31:50\n",
      "[epoch:   3 step: 16650] train loss: 0.000313046 time: 0:31:56\n",
      "[epoch:   3 step: 16700] train loss: 0.000303285 time: 0:32:02\n",
      "[epoch:   3 step: 16750] train loss: 0.000326956 time: 0:32:07\n",
      "[epoch:   3 step: 16800] train loss: 0.000257607 time: 0:32:13\n",
      "[epoch:   3 step: 16850] train loss: 0.000293614 time: 0:32:19\n",
      "[epoch:   3 step: 16900] train loss: 0.000303781 time: 0:32:25\n",
      "[epoch:   3 step: 16950] train loss: 0.000286524 time: 0:32:30\n",
      "[epoch:   3 step: 17000] train loss: 0.000277416 time: 0:32:36\n",
      "[epoch:   3 step: 17050] train loss: 0.000297118 time: 0:32:42\n",
      "[epoch:   3 step: 17100] train loss: 0.000307618 time: 0:32:48\n",
      "[epoch:   3 step: 17150] train loss: 0.000292108 time: 0:32:54\n",
      "[epoch:   3 step: 17200] train loss: 0.00029983 time: 0:33:00\n",
      "[epoch:   3 step: 17250] train loss: 0.000293432 time: 0:33:05\n",
      "[epoch:   3 step: 17300] train loss: 0.00030828 time: 0:33:11\n",
      "[epoch:   3 step: 17350] train loss: 0.000293547 time: 0:33:17\n",
      "[epoch:   3 step: 17400] train loss: 0.000306371 time: 0:33:23\n",
      "[epoch:   3 step: 17450] train loss: 0.000283287 time: 0:33:29\n",
      "[epoch:   3 step: 17500] train loss: 0.000308833 time: 0:33:35\n",
      "[epoch:   3 step: 17550] train loss: 0.000262167 time: 0:33:40\n",
      "[epoch:   3 step: 17600] train loss: 0.000301047 time: 0:33:46\n",
      "[epoch:   3 step: 17650] train loss: 0.000271127 time: 0:33:52\n",
      "[epoch:   3 step: 17700] train loss: 0.000276336 time: 0:33:58\n",
      "[epoch:   3 step: 17750] train loss: 0.000310629 time: 0:34:04\n",
      "[epoch:   3 step: 17800] train loss: 0.000270616 time: 0:34:09\n",
      "[epoch:   3 step: 17850] train loss: 0.000320625 time: 0:34:15\n",
      "[epoch:   3 step: 17900] train loss: 0.000286874 time: 0:34:21\n",
      "[epoch:   3 step: 17950] train loss: 0.000328923 time: 0:34:26\n",
      "[epoch:   3 step: 18000] train loss: 0.00030849 time: 0:34:32\n",
      "[epoch:   3 step: 18050] train loss: 0.000311074 time: 0:34:38\n",
      "[epoch:   3 step: 18100] train loss: 0.000317645 time: 0:34:44\n",
      "[epoch:   3 step: 18150] train loss: 0.000264851 time: 0:34:49\n",
      "[epoch:   3 step: 18200] train loss: 0.000281709 time: 0:34:55\n",
      "[epoch:   3 step: 18250] train loss: 0.000321559 time: 0:35:01\n",
      "[epoch:   3 step: 18300] train loss: 0.000308653 time: 0:35:07\n",
      "[epoch:   3 step: 18350] train loss: 0.000295423 time: 0:35:12\n",
      "[epoch:   3 step: 18400] train loss: 0.000308209 time: 0:35:18\n",
      "[epoch:   3 step: 18450] train loss: 0.00032519 time: 0:35:24\n",
      "[epoch:   3 step: 18500] train loss: 0.000323038 time: 0:35:30\n",
      "[epoch:   3 step: 18550] train loss: 0.000314569 time: 0:35:35\n",
      "[epoch:   3 step: 18600] train loss: 0.000289587 time: 0:35:41\n",
      "[epoch:   3 step: 18650] train loss: 0.000299362 time: 0:35:47\n",
      "[epoch:   3 step: 18700] train loss: 0.000314979 time: 0:35:53\n",
      "[epoch:   3 step: 18750] train loss: 0.000321674 time: 0:35:58\n",
      "[epoch:   4 step: 18800] train loss: 0.00026129 time: 0:36:04\n",
      "[epoch:   4 step: 18850] train loss: 0.000257964 time: 0:36:10\n",
      "[epoch:   4 step: 18900] train loss: 0.000234959 time: 0:36:16\n",
      "[epoch:   4 step: 18950] train loss: 0.000242447 time: 0:36:21\n",
      "[epoch:   4 step: 19000] train loss: 0.000269044 time: 0:36:27\n",
      "[epoch:   4 step: 19050] train loss: 0.000273463 time: 0:36:33\n",
      "[epoch:   4 step: 19100] train loss: 0.000260786 time: 0:36:38\n",
      "[epoch:   4 step: 19150] train loss: 0.000224156 time: 0:36:45\n",
      "[epoch:   4 step: 19200] train loss: 0.000229802 time: 0:36:50\n",
      "[epoch:   4 step: 19250] train loss: 0.00025427 time: 0:36:56\n",
      "[epoch:   4 step: 19300] train loss: 0.000208689 time: 0:37:02\n",
      "[epoch:   4 step: 19350] train loss: 0.000273375 time: 0:37:07\n",
      "[epoch:   4 step: 19400] train loss: 0.000286883 time: 0:37:13\n",
      "[epoch:   4 step: 19450] train loss: 0.000219177 time: 0:37:19\n",
      "[epoch:   4 step: 19500] train loss: 0.000254006 time: 0:37:25\n",
      "[epoch:   4 step: 19550] train loss: 0.000247756 time: 0:37:30\n",
      "[epoch:   4 step: 19600] train loss: 0.000242584 time: 0:37:36\n",
      "[epoch:   4 step: 19650] train loss: 0.000269105 time: 0:37:41\n",
      "[epoch:   4 step: 19700] train loss: 0.000242394 time: 0:37:47\n",
      "[epoch:   4 step: 19750] train loss: 0.000216821 time: 0:37:53\n",
      "[epoch:   4 step: 19800] train loss: 0.000245366 time: 0:37:59\n",
      "[epoch:   4 step: 19850] train loss: 0.000237493 time: 0:38:05\n",
      "[epoch:   4 step: 19900] train loss: 0.0002645 time: 0:38:11\n",
      "[epoch:   4 step: 19950] train loss: 0.000254633 time: 0:38:17\n",
      "[epoch:   4 step: 20000] train loss: 0.000249232 time: 0:38:22\n",
      "[epoch:   4 step: 20050] train loss: 0.000243532 time: 0:38:28\n",
      "[epoch:   4 step: 20100] train loss: 0.00024531 time: 0:38:34\n",
      "[epoch:   4 step: 20150] train loss: 0.000251541 time: 0:38:40\n",
      "[epoch:   4 step: 20200] train loss: 0.000272881 time: 0:38:45\n",
      "[epoch:   4 step: 20250] train loss: 0.000262025 time: 0:38:50\n",
      "[epoch:   4 step: 20300] train loss: 0.000283397 time: 0:38:56\n",
      "[epoch:   4 step: 20350] train loss: 0.000261242 time: 0:39:02\n",
      "[epoch:   4 step: 20400] train loss: 0.00025489 time: 0:39:08\n",
      "[epoch:   4 step: 20450] train loss: 0.000247427 time: 0:39:14\n",
      "[epoch:   4 step: 20500] train loss: 0.000270891 time: 0:39:20\n",
      "[epoch:   4 step: 20550] train loss: 0.000263376 time: 0:39:25\n",
      "[epoch:   4 step: 20600] train loss: 0.000258365 time: 0:39:31\n",
      "[epoch:   4 step: 20650] train loss: 0.000240896 time: 0:39:37\n",
      "[epoch:   4 step: 20700] train loss: 0.000235479 time: 0:39:43\n",
      "[epoch:   4 step: 20750] train loss: 0.000266571 time: 0:39:48\n",
      "[epoch:   4 step: 20800] train loss: 0.000263831 time: 0:39:54\n",
      "[epoch:   4 step: 20850] train loss: 0.000226035 time: 0:40:00\n",
      "[epoch:   4 step: 20900] train loss: 0.000277279 time: 0:40:06\n",
      "[epoch:   4 step: 20950] train loss: 0.000276837 time: 0:40:11\n",
      "[epoch:   4 step: 21000] train loss: 0.000266929 time: 0:40:17\n",
      "[epoch:   4 step: 21050] train loss: 0.000244368 time: 0:40:23\n",
      "[epoch:   4 step: 21100] train loss: 0.00024056 time: 0:40:28\n",
      "[epoch:   4 step: 21150] train loss: 0.000270966 time: 0:40:34\n",
      "[epoch:   4 step: 21200] train loss: 0.000249023 time: 0:40:40\n",
      "[epoch:   4 step: 21250] train loss: 0.000231154 time: 0:40:45\n",
      "[epoch:   4 step: 21300] train loss: 0.000235013 time: 0:40:51\n",
      "[epoch:   4 step: 21350] train loss: 0.000236287 time: 0:40:57\n",
      "[epoch:   4 step: 21400] train loss: 0.000272613 time: 0:41:02\n",
      "[epoch:   4 step: 21450] train loss: 0.000253001 time: 0:41:08\n",
      "[epoch:   4 step: 21500] train loss: 0.00028582 time: 0:41:14\n",
      "[epoch:   4 step: 21550] train loss: 0.000300447 time: 0:41:20\n",
      "[epoch:   4 step: 21600] train loss: 0.000270286 time: 0:41:25\n",
      "[epoch:   4 step: 21650] train loss: 0.00027747 time: 0:41:31\n",
      "[epoch:   4 step: 21700] train loss: 0.000255864 time: 0:41:36\n",
      "[epoch:   4 step: 21750] train loss: 0.000230415 time: 0:41:42\n",
      "[epoch:   4 step: 21800] train loss: 0.000238652 time: 0:41:48\n",
      "[epoch:   4 step: 21850] train loss: 0.000249909 time: 0:41:54\n",
      "[epoch:   4 step: 21900] train loss: 0.000252989 time: 0:42:00\n",
      "[epoch:   4 step: 21950] train loss: 0.000242787 time: 0:42:06\n",
      "[epoch:   4 step: 22000] train loss: 0.000245402 time: 0:42:12\n",
      "[epoch:   4 step: 22050] train loss: 0.000246877 time: 0:42:17\n",
      "[epoch:   4 step: 22100] train loss: 0.000232607 time: 0:42:23\n",
      "[epoch:   4 step: 22150] train loss: 0.000263325 time: 0:42:29\n",
      "[epoch:   4 step: 22200] train loss: 0.00026988 time: 0:42:35\n",
      "[epoch:   4 step: 22250] train loss: 0.000295305 time: 0:42:41\n",
      "[epoch:   4 step: 22300] train loss: 0.000268664 time: 0:42:47\n",
      "[epoch:   4 step: 22350] train loss: 0.000264317 time: 0:42:52\n",
      "[epoch:   4 step: 22400] train loss: 0.000258729 time: 0:42:58\n",
      "[epoch:   4 step: 22450] train loss: 0.00028028 time: 0:43:04\n",
      "[epoch:   4 step: 22500] train loss: 0.00026215 time: 0:43:10\n",
      "[epoch:   4 step: 22550] train loss: 0.000259499 time: 0:43:15\n",
      "[epoch:   4 step: 22600] train loss: 0.000284278 time: 0:43:21\n",
      "[epoch:   4 step: 22650] train loss: 0.000284118 time: 0:43:27\n",
      "[epoch:   4 step: 22700] train loss: 0.000243802 time: 0:43:32\n",
      "[epoch:   4 step: 22750] train loss: 0.000255578 time: 0:43:38\n",
      "[epoch:   4 step: 22800] train loss: 0.000250179 time: 0:43:44\n",
      "[epoch:   4 step: 22850] train loss: 0.000261207 time: 0:43:50\n",
      "[epoch:   4 step: 22900] train loss: 0.00025244 time: 0:43:56\n",
      "[epoch:   4 step: 22950] train loss: 0.000276738 time: 0:44:01\n",
      "[epoch:   4 step: 23000] train loss: 0.000230681 time: 0:44:07\n",
      "[epoch:   4 step: 23050] train loss: 0.000287023 time: 0:44:13\n",
      "[epoch:   4 step: 23100] train loss: 0.000277067 time: 0:44:19\n",
      "[epoch:   4 step: 23150] train loss: 0.000313101 time: 0:44:25\n",
      "[epoch:   4 step: 23200] train loss: 0.000267027 time: 0:44:31\n",
      "[epoch:   4 step: 23250] train loss: 0.000250592 time: 0:44:36\n",
      "[epoch:   4 step: 23300] train loss: 0.000238052 time: 0:44:42\n",
      "[epoch:   4 step: 23350] train loss: 0.000250342 time: 0:44:48\n",
      "[epoch:   4 step: 23400] train loss: 0.000296792 time: 0:44:53\n",
      "[epoch:   4 step: 23450] train loss: 0.000232246 time: 0:44:59\n",
      "[epoch:   4 step: 23500] train loss: 0.000270735 time: 0:45:04\n",
      "[epoch:   4 step: 23550] train loss: 0.000291248 time: 0:45:10\n",
      "[epoch:   4 step: 23600] train loss: 0.000247592 time: 0:45:16\n",
      "[epoch:   4 step: 23650] train loss: 0.000214496 time: 0:45:22\n",
      "[epoch:   4 step: 23700] train loss: 0.000280295 time: 0:45:27\n",
      "[epoch:   4 step: 23750] train loss: 0.000279029 time: 0:45:33\n",
      "[epoch:   4 step: 23800] train loss: 0.000260733 time: 0:45:39\n",
      "[epoch:   4 step: 23850] train loss: 0.000240833 time: 0:45:44\n",
      "[epoch:   4 step: 23900] train loss: 0.000271176 time: 0:45:50\n",
      "[epoch:   4 step: 23950] train loss: 0.000275652 time: 0:45:56\n",
      "[epoch:   4 step: 24000] train loss: 0.000247466 time: 0:46:02\n",
      "[epoch:   4 step: 24050] train loss: 0.000270722 time: 0:46:08\n",
      "[epoch:   4 step: 24100] train loss: 0.000270957 time: 0:46:13\n",
      "[epoch:   4 step: 24150] train loss: 0.000336574 time: 0:46:19\n",
      "[epoch:   4 step: 24200] train loss: 0.00027261 time: 0:46:25\n",
      "[epoch:   4 step: 24250] train loss: 0.000253815 time: 0:46:31\n",
      "[epoch:   4 step: 24300] train loss: 0.000274895 time: 0:46:37\n",
      "[epoch:   4 step: 24350] train loss: 0.000275682 time: 0:46:42\n",
      "[epoch:   4 step: 24400] train loss: 0.000279413 time: 0:46:48\n",
      "[epoch:   4 step: 24450] train loss: 0.000265945 time: 0:46:54\n",
      "[epoch:   4 step: 24500] train loss: 0.000253557 time: 0:46:59\n",
      "[epoch:   4 step: 24550] train loss: 0.000250334 time: 0:47:05\n",
      "[epoch:   4 step: 24600] train loss: 0.000250686 time: 0:47:11\n",
      "[epoch:   4 step: 24650] train loss: 0.000245544 time: 0:47:17\n",
      "[epoch:   4 step: 24700] train loss: 0.000241257 time: 0:47:23\n",
      "[epoch:   4 step: 24750] train loss: 0.000271929 time: 0:47:29\n",
      "[epoch:   4 step: 24800] train loss: 0.000277824 time: 0:47:34\n",
      "[epoch:   4 step: 24850] train loss: 0.000297358 time: 0:47:40\n",
      "[epoch:   4 step: 24900] train loss: 0.000252482 time: 0:47:46\n",
      "[epoch:   4 step: 24950] train loss: 0.000229303 time: 0:47:52\n",
      "[epoch:   4 step: 25000] train loss: 0.0002481 time: 0:47:57\n"
     ]
    }
   ],
   "source": [
    "!python train_BERTScore.py --model_type bert-base-uncased  --adapter_name debiased-bertscore --lr 5e-4 --warmup 0.0 --batch_size 16  --n_epochs 4  --seed 42  --device cuda --logging_steps 300  --data_path bertBase.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3a5c9-5023-430f-8827-6e76d9b5238e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
