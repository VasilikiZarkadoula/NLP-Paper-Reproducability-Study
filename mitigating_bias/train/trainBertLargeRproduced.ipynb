{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b58ada2f-0fc4-44a7-8a14-9644225409e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: adapter_transformers==3.1.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 1)) (3.1.0)\n",
      "Collecting fastNLP==0.7.0\n",
      "  Downloading FastNLP-0.7.0.tar.gz (295 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.2/295.2 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fitlog==0.9.13\n",
      "  Downloading fitlog-0.9.13.tar.gz (925 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.2/925.2 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib==3.4.3 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 4)) (3.4.3)\n",
      "Requirement already satisfied: numpy==1.20.3 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 5)) (1.20.3)\n",
      "Requirement already satisfied: pandas==1.3.4 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 6)) (1.3.4)\n",
      "Collecting sacrebleu==2.2.1\n",
      "  Downloading sacrebleu-2.2.1-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy==1.7.1\n",
      "  Downloading scipy-1.7.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.5/28.5 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting setuptools==58.0.4\n",
      "  Downloading setuptools-58.0.4-py3-none-any.whl (816 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m816.5/816.5 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm==4.62.3 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 10)) (4.62.3)\n",
      "Requirement already satisfied: transformers==4.23.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 11)) (4.23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (0.11.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (2022.7.9)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (2.28.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from adapter_transformers==3.1.0->-r requirements.txt (line 1)) (3.7.1)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from fastNLP==0.7.0->-r requirements.txt (line 2)) (1.12.0+cu116)\n",
      "Requirement already satisfied: prettytable>=0.7.2 in /usr/local/lib/python3.9/dist-packages (from fastNLP==0.7.0->-r requirements.txt (line 2)) (3.4.1)\n",
      "Collecting docopt>=0.6.2\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting flask>=1.0.2\n",
      "  Downloading Flask-2.2.2-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gitpython>=3.1.2 in /usr/local/lib/python3.9/dist-packages (from fitlog==0.9.13->-r requirements.txt (line 3)) (3.1.27)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.4.3->-r requirements.txt (line 4)) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.4.3->-r requirements.txt (line 4)) (1.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.4.3->-r requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.4.3->-r requirements.txt (line 4)) (9.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.4.3->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas==1.3.4->-r requirements.txt (line 6)) (2022.1)\n",
      "Requirement already satisfied: colorama in /usr/lib/python3/dist-packages (from sacrebleu==2.2.1->-r requirements.txt (line 7)) (0.4.3)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: lxml in /usr/lib/python3/dist-packages (from sacrebleu==2.2.1->-r requirements.txt (line 7)) (4.5.0)\n",
      "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.9/dist-packages (from flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (8.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.9/dist-packages (from flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (4.12.0)\n",
      "Collecting Werkzeug>=2.2.2\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.7/232.7 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting itsdangerous>=2.0\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from gitpython>=3.1.2->fitlog==0.9.13->-r requirements.txt (line 3)) (4.0.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (4.3.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prettytable>=0.7.2->fastNLP==0.7.0->-r requirements.txt (line 2)) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib==3.4.3->-r requirements.txt (line 4)) (1.14.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->adapter_transformers==3.1.0->-r requirements.txt (line 1)) (2.8)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.2->fitlog==0.9.13->-r requirements.txt (line 3)) (5.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6.0->flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (3.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from Jinja2>=3.0->flask>=1.0.2->fitlog==0.9.13->-r requirements.txt (line 3)) (2.1.1)\n",
      "Building wheels for collected packages: fastNLP, fitlog, docopt\n",
      "  Building wheel for fastNLP (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fastNLP: filename=FastNLP-0.7.0-py3-none-any.whl size=364728 sha256=baa20d105faa85f82341783ad7388512bdac5116f7a66bc6c68e82f8355b100a\n",
      "  Stored in directory: /root/.cache/pip/wheels/41/9e/16/22237f986720cce076fdc608c2bb6bc585326f221247e879db\n",
      "  Building wheel for fitlog (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fitlog: filename=fitlog-0.9.13-py3-none-any.whl size=967466 sha256=d6e1910cf245a726f0b5ce5ca3d4c4700c7bb4562e8a4b5db85a3696fbdd0ddf\n",
      "  Stored in directory: /root/.cache/pip/wheels/c7/52/61/d760755a6fbb06346a5fd541e5dc7021e7f9ee9cbe6c67edd1\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=3849858433fa23247b3b0d821e8fd5fe7a4a67857e3942ba5f6b7b7abb1ec0be\n",
      "  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built fastNLP fitlog docopt\n",
      "Installing collected packages: docopt, Werkzeug, tabulate, setuptools, scipy, portalocker, itsdangerous, sacrebleu, flask, fastNLP, fitlog\n",
      "  Attempting uninstall: Werkzeug\n",
      "    Found existing installation: Werkzeug 2.1.2\n",
      "    Uninstalling Werkzeug-2.1.2:\n",
      "      Successfully uninstalled Werkzeug-2.1.2\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 63.1.0\n",
      "    Uninstalling setuptools-63.1.0:\n",
      "      Successfully uninstalled setuptools-63.1.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.8.1\n",
      "    Uninstalling scipy-1.8.1:\n",
      "      Successfully uninstalled scipy-1.8.1\n",
      "Successfully installed Werkzeug-2.2.2 docopt-0.6.2 fastNLP-0.7.0 fitlog-0.9.13 flask-2.2.2 itsdangerous-2.1.2 portalocker-2.6.0 sacrebleu-2.2.1 scipy-1.7.1 setuptools-58.0.4 tabulate-0.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02db4c13-2b29-4df7-8835-d98bcb2ac7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.12.0+cu116)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.13.0+cu116)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (0.12.0+cu116)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.20.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision) (2019.11.28)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a37cd993-5d8c-47f4-9033-491e7f296c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertAdapterModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Read cache from cached_data.bin.\n",
      "# samples: 99999\n",
      "Example:\n",
      "+----------------+----------------+----------------+--------+\n",
      "| refs           | hyps           | labels         | type   |\n",
      "+----------------+----------------+----------------+--------+\n",
      "| a woman is ... | a person is... | 0.898857295... | debias |\n",
      "|                |                |                |        |\n",
      "+----------------+----------------+----------------+--------+\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "input fields after batch(if batch size is 2):\n",
      "\trefs: (1)type:numpy.ndarray (2)dtype:<U32, (3)shape:(2,) \n",
      "\thyps: (1)type:numpy.ndarray (2)dtype:<U33, (3)shape:(2,) \n",
      "\tlabels: (1)type:torch.Tensor (2)dtype:torch.float32, (3)shape:torch.Size([2]) \n",
      "target fields after batch(if batch size is 2):\n",
      "\tlabels: (1)type:torch.Tensor (2)dtype:torch.float32, (3)shape:torch.Size([2]) \n",
      "\n",
      "training epochs started 2023-01-13-14-11-46-517261\n",
      "[epoch:   1 step:   50] train loss: 0.0120096 time: 0:00:53\n",
      "[epoch:   1 step:  100] train loss: 0.00689289 time: 0:01:42\n",
      "[epoch:   1 step:  150] train loss: 0.00511332 time: 0:02:33\n",
      "[epoch:   1 step:  200] train loss: 0.00405639 time: 0:03:27\n",
      "[epoch:   1 step:  250] train loss: 0.00346279 time: 0:04:20\n",
      "[epoch:   1 step:  300] train loss: 0.00294713 time: 0:05:13\n",
      "[epoch:   1 step:  350] train loss: 0.00274393 time: 0:06:06\n",
      "[epoch:   1 step:  400] train loss: 0.00242886 time: 0:06:58\n",
      "[epoch:   1 step:  450] train loss: 0.00232489 time: 0:07:48\n",
      "[epoch:   1 step:  500] train loss: 0.00227492 time: 0:08:42\n",
      "[epoch:   1 step:  550] train loss: 0.00217266 time: 0:09:33\n",
      "[epoch:   1 step:  600] train loss: 0.00198093 time: 0:10:24\n",
      "[epoch:   1 step:  650] train loss: 0.00173361 time: 0:11:19\n",
      "[epoch:   1 step:  700] train loss: 0.0015403 time: 0:12:09\n",
      "[epoch:   1 step:  750] train loss: 0.00177618 time: 0:13:03\n",
      "[epoch:   1 step:  800] train loss: 0.00155081 time: 0:13:55\n",
      "[epoch:   1 step:  850] train loss: 0.00151386 time: 0:14:47\n",
      "[epoch:   1 step:  900] train loss: 0.00150401 time: 0:15:36\n",
      "[epoch:   1 step:  950] train loss: 0.00150704 time: 0:16:26\n",
      "[epoch:   1 step: 1000] train loss: 0.00145102 time: 0:17:18\n",
      "[epoch:   1 step: 1050] train loss: 0.00149976 time: 0:18:08\n",
      "[epoch:   1 step: 1100] train loss: 0.00134512 time: 0:18:59\n",
      "[epoch:   1 step: 1150] train loss: 0.00137844 time: 0:19:49\n",
      "[epoch:   1 step: 1200] train loss: 0.00133973 time: 0:20:44\n",
      "[epoch:   1 step: 1250] train loss: 0.00131245 time: 0:21:39\n",
      "[epoch:   1 step: 1300] train loss: 0.00119776 time: 0:22:30\n",
      "[epoch:   1 step: 1350] train loss: 0.00115451 time: 0:23:23\n",
      "[epoch:   1 step: 1400] train loss: 0.00112044 time: 0:24:19\n",
      "[epoch:   1 step: 1450] train loss: 0.00113729 time: 0:25:08\n",
      "[epoch:   1 step: 1500] train loss: 0.00117984 time: 0:25:58\n",
      "[epoch:   1 step: 1550] train loss: 0.00116963 time: 0:26:53\n",
      "[epoch:   1 step: 1600] train loss: 0.00103095 time: 0:27:43\n",
      "[epoch:   1 step: 1650] train loss: 0.00121111 time: 0:28:34\n",
      "[epoch:   1 step: 1700] train loss: 0.00101371 time: 0:29:25\n",
      "[epoch:   1 step: 1750] train loss: 0.00100522 time: 0:30:15\n",
      "[epoch:   1 step: 1800] train loss: 0.00108778 time: 0:31:04\n",
      "[epoch:   1 step: 1850] train loss: 0.000936539 time: 0:31:55\n",
      "[epoch:   1 step: 1900] train loss: 0.00100914 time: 0:32:46\n",
      "[epoch:   1 step: 1950] train loss: 0.00107396 time: 0:33:37\n",
      "[epoch:   1 step: 2000] train loss: 0.00110965 time: 0:34:31\n",
      "[epoch:   1 step: 2050] train loss: 0.00104998 time: 0:35:25\n",
      "[epoch:   1 step: 2100] train loss: 0.00102787 time: 0:36:14\n",
      "[epoch:   1 step: 2150] train loss: 0.00112071 time: 0:37:01\n",
      "[epoch:   1 step: 2200] train loss: 0.00106137 time: 0:37:50\n",
      "[epoch:   1 step: 2250] train loss: 0.00097446 time: 0:38:44\n",
      "[epoch:   1 step: 2300] train loss: 0.000967375 time: 0:39:36\n",
      "[epoch:   1 step: 2350] train loss: 0.000966127 time: 0:40:27\n",
      "[epoch:   1 step: 2400] train loss: 0.00101704 time: 0:41:18\n",
      "[epoch:   1 step: 2450] train loss: 0.000921657 time: 0:42:06\n",
      "[epoch:   1 step: 2500] train loss: 0.000984807 time: 0:42:57\n",
      "[epoch:   1 step: 2550] train loss: 0.000883118 time: 0:43:48\n",
      "[epoch:   1 step: 2600] train loss: 0.000821161 time: 0:44:40\n",
      "[epoch:   1 step: 2650] train loss: 0.000831797 time: 0:45:28\n",
      "[epoch:   1 step: 2700] train loss: 0.000904348 time: 0:46:23\n",
      "[epoch:   1 step: 2750] train loss: 0.000906489 time: 0:47:17\n",
      "[epoch:   1 step: 2800] train loss: 0.000883604 time: 0:48:04\n",
      "[epoch:   1 step: 2850] train loss: 0.00077447 time: 0:49:01\n",
      "[epoch:   1 step: 2900] train loss: 0.00088415 time: 0:49:52\n",
      "[epoch:   1 step: 2950] train loss: 0.000822246 time: 0:50:40\n",
      "[epoch:   1 step: 3000] train loss: 0.000860899 time: 0:51:31\n",
      "[epoch:   1 step: 3050] train loss: 0.000833667 time: 0:52:22\n",
      "[epoch:   1 step: 3100] train loss: 0.0007681 time: 0:53:13\n",
      "[epoch:   2 step: 3150] train loss: 0.000733281 time: 0:54:03\n",
      "[epoch:   2 step: 3200] train loss: 0.000794107 time: 0:54:58\n",
      "[epoch:   2 step: 3250] train loss: 0.000770648 time: 0:55:52\n",
      "[epoch:   2 step: 3300] train loss: 0.000704821 time: 0:56:45\n",
      "[epoch:   2 step: 3350] train loss: 0.000760521 time: 0:57:39\n",
      "[epoch:   2 step: 3400] train loss: 0.000714577 time: 0:58:29\n",
      "[epoch:   2 step: 3450] train loss: 0.000763819 time: 0:59:20\n",
      "[epoch:   2 step: 3500] train loss: 0.00073643 time: 1:00:14\n",
      "[epoch:   2 step: 3550] train loss: 0.000956506 time: 1:01:10\n",
      "[epoch:   2 step: 3600] train loss: 0.00072565 time: 1:02:05\n",
      "[epoch:   2 step: 3650] train loss: 0.000773713 time: 1:02:54\n",
      "[epoch:   2 step: 3700] train loss: 0.000745873 time: 1:03:44\n",
      "[epoch:   2 step: 3750] train loss: 0.000698543 time: 1:04:35\n",
      "[epoch:   2 step: 3800] train loss: 0.000680635 time: 1:05:24\n",
      "[epoch:   2 step: 3850] train loss: 0.000712615 time: 1:06:14\n",
      "[epoch:   2 step: 3900] train loss: 0.000675298 time: 1:07:05\n",
      "[epoch:   2 step: 3950] train loss: 0.000719618 time: 1:07:55\n",
      "[epoch:   2 step: 4000] train loss: 0.000727579 time: 1:08:45\n",
      "[epoch:   2 step: 4050] train loss: 0.00067653 time: 1:09:37\n",
      "[epoch:   2 step: 4100] train loss: 0.000641284 time: 1:10:27\n",
      "[epoch:   2 step: 4150] train loss: 0.000621717 time: 1:11:21\n",
      "[epoch:   2 step: 4200] train loss: 0.000731234 time: 1:12:10\n",
      "[epoch:   2 step: 4250] train loss: 0.000682985 time: 1:13:03\n",
      "[epoch:   2 step: 4300] train loss: 0.000616234 time: 1:13:59\n",
      "[epoch:   2 step: 4350] train loss: 0.000675086 time: 1:14:48\n",
      "[epoch:   2 step: 4400] train loss: 0.000659397 time: 1:15:43\n",
      "[epoch:   2 step: 4450] train loss: 0.000715721 time: 1:16:33\n",
      "[epoch:   2 step: 4500] train loss: 0.000790835 time: 1:17:27\n",
      "[epoch:   2 step: 4550] train loss: 0.000712362 time: 1:18:18\n",
      "[epoch:   2 step: 4600] train loss: 0.000634218 time: 1:19:12\n",
      "[epoch:   2 step: 4650] train loss: 0.00067941 time: 1:20:02\n",
      "[epoch:   2 step: 4700] train loss: 0.000644418 time: 1:20:58\n",
      "[epoch:   2 step: 4750] train loss: 0.000609036 time: 1:21:47\n",
      "[epoch:   2 step: 4800] train loss: 0.000647091 time: 1:22:38\n",
      "[epoch:   2 step: 4850] train loss: 0.000638521 time: 1:23:29\n",
      "[epoch:   2 step: 4900] train loss: 0.000653572 time: 1:24:21\n",
      "[epoch:   2 step: 4950] train loss: 0.000672994 time: 1:25:15\n",
      "[epoch:   2 step: 5000] train loss: 0.000620877 time: 1:26:08\n",
      "[epoch:   2 step: 5050] train loss: 0.00068002 time: 1:27:00\n",
      "[epoch:   2 step: 5100] train loss: 0.000579113 time: 1:27:50\n",
      "[epoch:   2 step: 5150] train loss: 0.000645385 time: 1:28:44\n",
      "[epoch:   2 step: 5200] train loss: 0.000618555 time: 1:29:36\n",
      "[epoch:   2 step: 5250] train loss: 0.000709387 time: 1:30:25\n",
      "[epoch:   2 step: 5300] train loss: 0.000671316 time: 1:31:15\n",
      "[epoch:   2 step: 5350] train loss: 0.000637868 time: 1:32:03\n",
      "[epoch:   2 step: 5400] train loss: 0.000573514 time: 1:32:52\n",
      "[epoch:   2 step: 5450] train loss: 0.00060516 time: 1:33:43\n",
      "[epoch:   2 step: 5500] train loss: 0.000573529 time: 1:34:37\n",
      "[epoch:   2 step: 5550] train loss: 0.000546854 time: 1:35:28\n",
      "[epoch:   2 step: 5600] train loss: 0.00058481 time: 1:36:19\n",
      "[epoch:   2 step: 5650] train loss: 0.00064254 time: 1:37:14\n",
      "[epoch:   2 step: 5700] train loss: 0.000573693 time: 1:38:05\n",
      "[epoch:   2 step: 5750] train loss: 0.00065853 time: 1:38:57\n",
      "[epoch:   2 step: 5800] train loss: 0.000601942 time: 1:39:48\n",
      "[epoch:   2 step: 5850] train loss: 0.000627357 time: 1:40:35\n",
      "[epoch:   2 step: 5900] train loss: 0.000655485 time: 1:41:25\n",
      "[epoch:   2 step: 5950] train loss: 0.000738681 time: 1:42:16\n",
      "[epoch:   2 step: 6000] train loss: 0.00071313 time: 1:43:08\n",
      "[epoch:   2 step: 6050] train loss: 0.000617191 time: 1:44:02\n",
      "[epoch:   2 step: 6100] train loss: 0.00060797 time: 1:44:56\n",
      "[epoch:   2 step: 6150] train loss: 0.000574554 time: 1:45:46\n",
      "[epoch:   2 step: 6200] train loss: 0.000540724 time: 1:46:40\n",
      "[epoch:   2 step: 6250] train loss: 0.000556191 time: 1:47:37\n",
      "[epoch:   3 step: 6300] train loss: 0.00053196 time: 1:48:29\n",
      "[epoch:   3 step: 6350] train loss: 0.000508859 time: 1:49:24\n",
      "[epoch:   3 step: 6400] train loss: 0.00054528 time: 1:50:14\n",
      "[epoch:   3 step: 6450] train loss: 0.000512366 time: 1:51:05\n",
      "[epoch:   3 step: 6500] train loss: 0.000515911 time: 1:52:00\n",
      "[epoch:   3 step: 6550] train loss: 0.000472717 time: 1:52:51\n",
      "[epoch:   3 step: 6600] train loss: 0.000496619 time: 1:53:44\n",
      "[epoch:   3 step: 6650] train loss: 0.00055219 time: 1:54:36\n",
      "[epoch:   3 step: 6700] train loss: 0.000505148 time: 1:55:26\n",
      "[epoch:   3 step: 6750] train loss: 0.000576523 time: 1:56:20\n",
      "[epoch:   3 step: 6800] train loss: 0.000531511 time: 1:57:08\n",
      "[epoch:   3 step: 6850] train loss: 0.000522766 time: 1:58:06\n",
      "[epoch:   3 step: 6900] train loss: 0.000496036 time: 1:58:56\n",
      "[epoch:   3 step: 6950] train loss: 0.000524017 time: 1:59:50\n",
      "[epoch:   3 step: 7000] train loss: 0.000518792 time: 2:00:39\n",
      "[epoch:   3 step: 7050] train loss: 0.000514308 time: 2:01:28\n",
      "[epoch:   3 step: 7100] train loss: 0.000527052 time: 2:02:22\n",
      "[epoch:   3 step: 7150] train loss: 0.000513561 time: 2:03:10\n",
      "[epoch:   3 step: 7200] train loss: 0.000510134 time: 2:03:59\n",
      "[epoch:   3 step: 7250] train loss: 0.0004653 time: 2:04:54\n",
      "[epoch:   3 step: 7300] train loss: 0.000465269 time: 2:05:44\n",
      "[epoch:   3 step: 7350] train loss: 0.00044424 time: 2:06:35\n",
      "[epoch:   3 step: 7400] train loss: 0.000535823 time: 2:07:27\n",
      "[epoch:   3 step: 7450] train loss: 0.000503606 time: 2:08:16\n",
      "[epoch:   3 step: 7500] train loss: 0.000554372 time: 2:09:09\n",
      "[epoch:   3 step: 7550] train loss: 0.000482416 time: 2:10:03\n",
      "[epoch:   3 step: 7600] train loss: 0.000499429 time: 2:10:52\n",
      "[epoch:   3 step: 7650] train loss: 0.000557289 time: 2:11:44\n",
      "[epoch:   3 step: 7700] train loss: 0.000504272 time: 2:12:34\n",
      "[epoch:   3 step: 7750] train loss: 0.000466891 time: 2:13:30\n",
      "[epoch:   3 step: 7800] train loss: 0.000484711 time: 2:14:23\n",
      "[epoch:   3 step: 7850] train loss: 0.000515374 time: 2:15:16\n",
      "[epoch:   3 step: 7900] train loss: 0.000477658 time: 2:16:09\n",
      "[epoch:   3 step: 7950] train loss: 0.000513304 time: 2:16:59\n",
      "[epoch:   3 step: 8000] train loss: 0.000524463 time: 2:17:53\n",
      "[epoch:   3 step: 8050] train loss: 0.000495653 time: 2:18:43\n",
      "[epoch:   3 step: 8100] train loss: 0.000479099 time: 2:19:35\n",
      "[epoch:   3 step: 8150] train loss: 0.000481299 time: 2:20:24\n",
      "[epoch:   3 step: 8200] train loss: 0.000492103 time: 2:21:18\n",
      "[epoch:   3 step: 8250] train loss: 0.000520301 time: 2:22:09\n",
      "[epoch:   3 step: 8300] train loss: 0.000496009 time: 2:22:58\n",
      "[epoch:   3 step: 8350] train loss: 0.000456454 time: 2:23:49\n",
      "[epoch:   3 step: 8400] train loss: 0.000458315 time: 2:24:39\n",
      "[epoch:   3 step: 8450] train loss: 0.000472998 time: 2:25:32\n",
      "[epoch:   3 step: 8500] train loss: 0.000489594 time: 2:26:25\n",
      "[epoch:   3 step: 8550] train loss: 0.000472793 time: 2:27:20\n",
      "[epoch:   3 step: 8600] train loss: 0.000451191 time: 2:28:14\n",
      "[epoch:   3 step: 8650] train loss: 0.000478161 time: 2:29:07\n",
      "[epoch:   3 step: 8700] train loss: 0.000453013 time: 2:30:00\n",
      "[epoch:   3 step: 8750] train loss: 0.000410267 time: 2:30:53\n",
      "[epoch:   3 step: 8800] train loss: 0.000487218 time: 2:31:43\n",
      "[epoch:   3 step: 8850] train loss: 0.000451847 time: 2:32:37\n",
      "[epoch:   3 step: 8900] train loss: 0.000455562 time: 2:33:29\n",
      "[epoch:   3 step: 8950] train loss: 0.000457349 time: 2:34:21\n",
      "[epoch:   3 step: 9000] train loss: 0.00044741 time: 2:35:12\n",
      "[epoch:   3 step: 9050] train loss: 0.000432121 time: 2:36:02\n",
      "[epoch:   3 step: 9100] train loss: 0.000410881 time: 2:36:53\n",
      "[epoch:   3 step: 9150] train loss: 0.00047642 time: 2:37:43\n",
      "[epoch:   3 step: 9200] train loss: 0.000487231 time: 2:38:35\n",
      "[epoch:   3 step: 9250] train loss: 0.00049131 time: 2:39:29\n",
      "[epoch:   3 step: 9300] train loss: 0.000476697 time: 2:40:19\n",
      "[epoch:   3 step: 9350] train loss: 0.000440084 time: 2:41:13\n",
      "[epoch:   4 step: 9400] train loss: 0.000436891 time: 2:42:04\n",
      "[epoch:   4 step: 9450] train loss: 0.000430341 time: 2:42:54\n",
      "[epoch:   4 step: 9500] train loss: 0.000437844 time: 2:43:46\n",
      "[epoch:   4 step: 9550] train loss: 0.000423076 time: 2:44:36\n",
      "[epoch:   4 step: 9600] train loss: 0.000442492 time: 2:45:32\n",
      "[epoch:   4 step: 9650] train loss: 0.000372161 time: 2:46:26\n",
      "[epoch:   4 step: 9700] train loss: 0.00044353 time: 2:47:17\n",
      "[epoch:   4 step: 9750] train loss: 0.000418957 time: 2:48:03\n",
      "[epoch:   4 step: 9800] train loss: 0.00039608 time: 2:48:55\n",
      "[epoch:   4 step: 9850] train loss: 0.000442455 time: 2:49:46\n",
      "[epoch:   4 step: 9900] train loss: 0.000448476 time: 2:50:38\n",
      "[epoch:   4 step: 9950] train loss: 0.000420559 time: 2:51:32\n",
      "[epoch:   4 step: 10000] train loss: 0.000439996 time: 2:52:22\n",
      "[epoch:   4 step: 10050] train loss: 0.000392921 time: 2:53:13\n",
      "[epoch:   4 step: 10100] train loss: 0.000405287 time: 2:54:07\n",
      "[epoch:   4 step: 10150] train loss: 0.000429462 time: 2:54:56\n",
      "[epoch:   4 step: 10200] train loss: 0.000403485 time: 2:55:50\n",
      "[epoch:   4 step: 10250] train loss: 0.000407224 time: 2:56:40\n",
      "[epoch:   4 step: 10300] train loss: 0.000379925 time: 2:57:29\n",
      "[epoch:   4 step: 10350] train loss: 0.000371853 time: 2:58:24\n",
      "[epoch:   4 step: 10400] train loss: 0.000410156 time: 2:59:14\n",
      "[epoch:   4 step: 10450] train loss: 0.000375064 time: 3:00:08\n",
      "[epoch:   4 step: 10500] train loss: 0.000385467 time: 3:00:56\n",
      "[epoch:   4 step: 10550] train loss: 0.000431617 time: 3:01:45\n",
      "[epoch:   4 step: 10600] train loss: 0.00038879 time: 3:02:37\n",
      "[epoch:   4 step: 10650] train loss: 0.000378993 time: 3:03:27\n",
      "[epoch:   4 step: 10700] train loss: 0.000384203 time: 3:04:19\n",
      "[epoch:   4 step: 10750] train loss: 0.000413708 time: 3:05:12\n",
      "[epoch:   4 step: 10800] train loss: 0.000440478 time: 3:06:03\n",
      "[epoch:   4 step: 10850] train loss: 0.000375759 time: 3:06:52\n",
      "[epoch:   4 step: 10900] train loss: 0.000379568 time: 3:07:51\n",
      "[epoch:   4 step: 10950] train loss: 0.000395009 time: 3:08:45\n",
      "[epoch:   4 step: 11000] train loss: 0.000404646 time: 3:09:36\n",
      "[epoch:   4 step: 11050] train loss: 0.000323826 time: 3:10:28\n",
      "[epoch:   4 step: 11100] train loss: 0.000383313 time: 3:11:22\n",
      "[epoch:   4 step: 11150] train loss: 0.000383298 time: 3:12:11\n",
      "[epoch:   4 step: 11200] train loss: 0.000360328 time: 3:13:04\n",
      "[epoch:   4 step: 11250] train loss: 0.000360185 time: 3:13:58\n",
      "[epoch:   4 step: 11300] train loss: 0.000375477 time: 3:14:47\n",
      "[epoch:   4 step: 11350] train loss: 0.000413698 time: 3:15:41\n",
      "[epoch:   4 step: 11400] train loss: 0.000368671 time: 3:16:33\n",
      "[epoch:   4 step: 11450] train loss: 0.000393115 time: 3:17:28\n",
      "[epoch:   4 step: 11500] train loss: 0.00037365 time: 3:18:23\n",
      "[epoch:   4 step: 11550] train loss: 0.000379836 time: 3:19:18\n",
      "[epoch:   4 step: 11600] train loss: 0.000360603 time: 3:20:09\n",
      "[epoch:   4 step: 11650] train loss: 0.000371426 time: 3:21:01\n",
      "[epoch:   4 step: 11700] train loss: 0.000364768 time: 3:21:52\n",
      "[epoch:   4 step: 11750] train loss: 0.000346957 time: 3:22:41\n",
      "[epoch:   4 step: 11800] train loss: 0.00038391 time: 3:23:33\n",
      "[epoch:   4 step: 11850] train loss: 0.000378271 time: 3:24:25\n",
      "[epoch:   4 step: 11900] train loss: 0.000406929 time: 3:25:13\n",
      "[epoch:   4 step: 11950] train loss: 0.000369165 time: 3:26:03\n",
      "[epoch:   4 step: 12000] train loss: 0.000366697 time: 3:26:56\n",
      "[epoch:   4 step: 12050] train loss: 0.000425387 time: 3:27:51\n",
      "[epoch:   4 step: 12100] train loss: 0.000392243 time: 3:28:42\n",
      "[epoch:   4 step: 12150] train loss: 0.000355462 time: 3:29:32\n",
      "[epoch:   4 step: 12200] train loss: 0.000420569 time: 3:30:25\n",
      "[epoch:   4 step: 12250] train loss: 0.000390665 time: 3:31:15\n",
      "[epoch:   4 step: 12300] train loss: 0.000386696 time: 3:32:10\n",
      "[epoch:   4 step: 12350] train loss: 0.00037319 time: 3:33:02\n",
      "[epoch:   4 step: 12400] train loss: 0.000382329 time: 3:33:52\n",
      "[epoch:   4 step: 12450] train loss: 0.000426623 time: 3:34:45\n",
      "[epoch:   4 step: 12500] train loss: 0.000358296 time: 3:35:36\n"
     ]
    }
   ],
   "source": [
    "!python train_BERTScore.py --model_type bert-large-uncased  --adapter_name debiased-bertscore --lr 2e-4 --warmup 0.0 --batch_size 32  --n_epochs 4  --seed 42  --device cuda --logging_steps 150  --data_path bertLarge.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a810472-c952-4342-9fca-47ad8842a24c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
